{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Analysis: Sniper Optimization Runs 32 & 33\n",
    "\n",
    "**Run 32**: 115 model trials, original search spaces  \n",
    "**Run 33**: 75 model trials, expanded search spaces, fixed Optuna objective  \n",
    "\n",
    "**Bet types**: away_win, btts, cards, corners, fouls, home_win, over25, shots, under25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport yaml\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport seaborn as sns\nfrom scipy import stats\ntry:\n    display\nexcept NameError:\n    display = print  # fallback for non-Jupyter execution\n\npd.set_option('display.max_columns', 50)\npd.set_option('display.float_format', '{:.3f}'.format)\nsns.set_theme(style='whitegrid', palette='colorblind')\nplt.rcParams['figure.figsize'] = (14, 6)\nplt.rcParams['figure.dpi'] = 100\n\nBET_TYPES = ['away_win', 'btts', 'cards', 'corners', 'fouls', 'home_win', 'over25', 'shots', 'under25']\n# Resolve project root robustly (works from notebook CWD or project root)\n_cwd = Path.cwd()\n_PROJECT_ROOT = _cwd if (_cwd / 'data' / 'artifacts').exists() else _cwd / '..'\n_PROJECT_ROOT = _PROJECT_ROOT.resolve()\nRUNS = {32: _PROJECT_ROOT / 'data' / 'artifacts' / 'sniper-all-results-32',\n        33: _PROJECT_ROOT / 'data' / 'artifacts' / 'sniper-all-results-33'}\nprint(f'Project root: {_PROJECT_ROOT}')\nprint(f'Run 32 exists: {RUNS[32].exists()}, Run 33 exists: {RUNS[33].exists()}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_json(directory: Path, prefix: str) -> dict | None:\n",
    "    \"\"\"Load the latest JSON file matching prefix (by filename timestamp).\"\"\"\n",
    "    files = sorted(directory.glob(f'{prefix}*.json'))\n",
    "    if not files:\n",
    "        return None\n",
    "    with open(files[-1]) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_yaml(path: Path) -> dict | None:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with open(path) as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Load all data\n",
    "sniper = {}   # sniper[run][bet_type] = dict\n",
    "fparams = {}  # fparams[run][bet_type] = dict\n",
    "yamls = {}    # yamls[run][bet_type] = dict\n",
    "\n",
    "for run_id, run_dir in RUNS.items():\n",
    "    sniper[run_id] = {}\n",
    "    fparams[run_id] = {}\n",
    "    yamls[run_id] = {}\n",
    "    for bt in BET_TYPES:\n",
    "        sniper[run_id][bt] = load_latest_json(run_dir, f'sniper_{bt}_')\n",
    "        fparams[run_id][bt] = load_latest_json(run_dir, f'feature_params_{bt}_')\n",
    "        yamls[run_id][bt] = load_yaml(run_dir / 'feature_params' / f'{bt}.yaml')\n",
    "\n",
    "# Verify loading\n",
    "for run_id in [32, 33]:\n",
    "    loaded = [bt for bt in BET_TYPES if sniper[run_id][bt] is not None]\n",
    "    fp_loaded = [bt for bt in BET_TYPES if fparams[run_id][bt] is not None]\n",
    "    print(f'Run {run_id}: {len(loaded)} sniper results, {len(fp_loaded)} feature params')\n",
    "    if len(loaded) < 9:\n",
    "        print(f'  Missing sniper: {set(BET_TYPES) - set(loaded)}')\n",
    "    if len(fp_loaded) < 9:\n",
    "        print(f'  Missing fparams: {set(BET_TYPES) - set(fp_loaded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unified DataFrames\n",
    "\n",
    "# Sniper summary DataFrame\n",
    "rows = []\n",
    "for run_id in [32, 33]:\n",
    "    for bt in BET_TYPES:\n",
    "        s = sniper[run_id][bt]\n",
    "        if s is None:\n",
    "            continue\n",
    "        h = s.get('holdout_metrics', {})\n",
    "        row = {\n",
    "            'run': run_id, 'bet_type': bt,\n",
    "            'best_model': s.get('best_model'),\n",
    "            'n_features': s.get('n_features'),\n",
    "            'threshold': s.get('best_threshold'),\n",
    "            'min_odds': s.get('best_min_odds'),\n",
    "            'max_odds': s.get('best_max_odds'),\n",
    "            'bt_precision': s.get('precision'),\n",
    "            'bt_roi': s.get('roi'),\n",
    "            'bt_n_bets': s.get('n_bets'),\n",
    "            'holdout_precision': h.get('precision'),\n",
    "            'holdout_roi': h.get('roi'),\n",
    "            'holdout_n_bets': h.get('n_bets'),\n",
    "            'holdout_sharpe': h.get('sharpe'),\n",
    "            'holdout_sortino': h.get('sortino'),\n",
    "            'holdout_ece': h.get('ece'),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "df_sniper = pd.DataFrame(rows)\n",
    "print(f'Sniper summary: {len(df_sniper)} rows ({df_sniper.run.nunique()} runs × {df_sniper.bet_type.nunique()} bet types)')\n",
    "df_sniper.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature params summary DataFrame\n",
    "fp_rows = []\n",
    "for run_id in [32, 33]:\n",
    "    for bt in BET_TYPES:\n",
    "        fp = fparams[run_id][bt]\n",
    "        if fp is None:\n",
    "            continue\n",
    "        row = {'run': run_id, 'bet_type': bt,\n",
    "               'sharpe': fp.get('sharpe'), 'precision': fp.get('precision'),\n",
    "               'roi': fp.get('roi'), 'n_bets': fp.get('n_bets'),\n",
    "               'n_trials': fp.get('n_trials')}\n",
    "        for k, v in fp.get('best_params', {}).items():\n",
    "            row[f'param_{k}'] = v\n",
    "        fp_rows.append(row)\n",
    "\n",
    "df_fparams = pd.DataFrame(fp_rows)\n",
    "df_fparams.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Parameter Tuning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search space definitions\n",
    "SEARCH_SPACES = {\n",
    "    'elo_k_factor': (10, 50),\n",
    "    'elo_home_advantage': (25, 250),\n",
    "    'form_window': (3, 20),\n",
    "    'ema_span': (3, 20),\n",
    "    'poisson_lookback': (5, 40),\n",
    "    'fouls_ema_span': (3, 20),\n",
    "    'cards_ema_span': (3, 20),\n",
    "    'shots_ema_span': (3, 20),\n",
    "    'corners_ema_span': (3, 20),\n",
    "}\n",
    "\n",
    "# Boundary analysis: flag params within 10% of boundary\n",
    "boundary_rows = []\n",
    "for run_id in [32, 33]:\n",
    "    for bt in BET_TYPES:\n",
    "        fp = fparams[run_id][bt]\n",
    "        if fp is None:\n",
    "            continue\n",
    "        # Use search_space from the JSON if available, else global\n",
    "        ss = fp.get('search_space', {})\n",
    "        for param, value in fp.get('best_params', {}).items():\n",
    "            if param in ss:\n",
    "                lo, hi = ss[param][0], ss[param][1]\n",
    "            elif param in SEARCH_SPACES:\n",
    "                lo, hi = SEARCH_SPACES[param]\n",
    "            else:\n",
    "                continue\n",
    "            rng = hi - lo\n",
    "            near_lo = (value - lo) / rng <= 0.1\n",
    "            near_hi = (hi - value) / rng <= 0.1\n",
    "            boundary_rows.append({\n",
    "                'run': run_id, 'bet_type': bt, 'param': param,\n",
    "                'value': value, 'lo': lo, 'hi': hi,\n",
    "                'pct_of_range': (value - lo) / rng,\n",
    "                'at_boundary': 'LOW' if near_lo else ('HIGH' if near_hi else 'OK')\n",
    "            })\n",
    "\n",
    "df_boundary = pd.DataFrame(boundary_rows)\n",
    "flagged = df_boundary[df_boundary.at_boundary != 'OK'].sort_values(['param', 'bet_type', 'run'])\n",
    "print(f'=== Parameters at boundary (within 10% of min/max) ===')\n",
    "print(f'{len(flagged)} cases flagged out of {len(df_boundary)} total\\n')\n",
    "if len(flagged) > 0:\n",
    "    display(flagged[['run', 'bet_type', 'param', 'value', 'lo', 'hi', 'at_boundary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary visualization: best param values vs search space bounds\n",
    "core_params = ['elo_k_factor', 'form_window', 'ema_span', 'poisson_lookback']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "for ax, param in zip(axes.flat, core_params):\n",
    "    lo, hi = SEARCH_SPACES[param]\n",
    "    for run_id, marker in [(32, 'o'), (33, 's')]:\n",
    "        vals, labels = [], []\n",
    "        for bt in BET_TYPES:\n",
    "            fp = fparams[run_id][bt]\n",
    "            if fp and param in fp.get('best_params', {}):\n",
    "                vals.append(fp['best_params'][param])\n",
    "                labels.append(bt)\n",
    "        if vals:\n",
    "            ax.scatter(labels, vals, marker=marker, s=80, label=f'Run {run_id}', zorder=3)\n",
    "    ax.axhline(lo, color='red', ls='--', alpha=0.5, label='Search bounds')\n",
    "    ax.axhline(hi, color='red', ls='--', alpha=0.5)\n",
    "    # 10% boundary zones\n",
    "    rng = hi - lo\n",
    "    ax.axhspan(lo, lo + 0.1 * rng, color='red', alpha=0.08)\n",
    "    ax.axhspan(hi - 0.1 * rng, hi, color='red', alpha=0.08)\n",
    "    ax.set_title(param)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "fig.suptitle('Feature Parameter Best Values vs Search Space Bounds', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial convergence: Sharpe/ROI across trials per bet type\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "for ax, bt in zip(axes.flat, BET_TYPES):\n",
    "    for run_id, color in [(32, 'C0'), (33, 'C1')]:\n",
    "        fp = fparams[run_id][bt]\n",
    "        if fp is None or 'all_trials' not in fp:\n",
    "            continue\n",
    "        trials = fp['all_trials']\n",
    "        trial_nums = [t['number'] for t in trials]\n",
    "        rois = [t.get('roi', 0) for t in trials]\n",
    "        ax.plot(trial_nums, rois, marker='.', ms=4, alpha=0.7, color=color, label=f'R{run_id}')\n",
    "        # Running best\n",
    "        running_best = np.maximum.accumulate(rois)\n",
    "        ax.plot(trial_nums, running_best, ls='--', color=color, alpha=0.5)\n",
    "    ax.set_title(bt, fontsize=10)\n",
    "    ax.set_xlabel('Trial #', fontsize=8)\n",
    "    ax.set_ylabel('ROI %', fontsize=8)\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "fig.suptitle('Feature Param Optimization Convergence (solid=trial ROI, dashed=running best)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter distributions: violin plots of trial values per param\n",
    "trial_param_rows = []\n",
    "for run_id in [32, 33]:\n",
    "    for bt in BET_TYPES:\n",
    "        fp = fparams[run_id][bt]\n",
    "        if fp is None or 'all_trials' not in fp:\n",
    "            continue\n",
    "        for trial in fp['all_trials']:\n",
    "            for param, val in trial.get('params', {}).items():\n",
    "                trial_param_rows.append({\n",
    "                    'run': run_id, 'bet_type': bt, 'param': param, 'value': val,\n",
    "                    'trial': trial['number']\n",
    "                })\n",
    "\n",
    "df_trial_params = pd.DataFrame(trial_param_rows)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "for ax, param in zip(axes.flat, core_params):\n",
    "    subset = df_trial_params[df_trial_params.param == param]\n",
    "    if subset.empty:\n",
    "        ax.set_title(f'{param} (no data)')\n",
    "        continue\n",
    "    sns.violinplot(data=subset, x='bet_type', y='value', hue='run',\n",
    "                   split=True, inner='quart', ax=ax, palette='Set2')\n",
    "    lo, hi = SEARCH_SPACES.get(param, (None, None))\n",
    "    if lo is not None:\n",
    "        ax.axhline(lo, color='red', ls=':', alpha=0.4)\n",
    "        ax.axhline(hi, color='red', ls=':', alpha=0.4)\n",
    "    ax.set_title(param)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "fig.suptitle('Parameter Distributions Across Optuna Trials', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cross-bet-type param heatmap: best params per bet type\nparam_cols = [c for c in df_fparams.columns if c.startswith('param_')]\n\nfor run_id in [32, 33]:\n    sub = df_fparams[df_fparams.run == run_id].set_index('bet_type')[param_cols].dropna(axis=1, how='all')\n    # Normalize each param 0-1 for heatmap comparability\n    sub_norm = (sub - sub.min()) / (sub.max() - sub.min() + 1e-9)\n    sub_norm.columns = [c.replace('param_', '') for c in sub_norm.columns]\n\n    # Build raw annotation array\n    raw = sub.copy()\n    raw.columns = [c.replace('param_', '') for c in raw.columns]\n    annot = raw.fillna(0).values\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.heatmap(sub_norm, annot=annot, fmt='.0f', cmap='YlOrRd', ax=ax, linewidths=0.5)\n    ax.set_title(f'Run {run_id}: Best Feature Params (color=normalized, text=raw value)')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 32 → 33 delta table\n",
    "delta_rows = []\n",
    "for bt in BET_TYPES:\n",
    "    fp32 = fparams[32].get(bt)\n",
    "    fp33 = fparams[33].get(bt)\n",
    "    if fp32 is None or fp33 is None:\n",
    "        continue\n",
    "    p32 = fp32.get('best_params', {})\n",
    "    p33 = fp33.get('best_params', {})\n",
    "    all_params = set(p32.keys()) | set(p33.keys())\n",
    "    for param in sorted(all_params):\n",
    "        v32 = p32.get(param)\n",
    "        v33 = p33.get(param)\n",
    "        if v32 is not None and v33 is not None:\n",
    "            delta = v33 - v32\n",
    "            pct_delta = delta / (abs(v32) + 1e-9) * 100\n",
    "        else:\n",
    "            delta = None\n",
    "            pct_delta = None\n",
    "        delta_rows.append({\n",
    "            'bet_type': bt, 'param': param,\n",
    "            'run32': v32, 'run33': v33,\n",
    "            'delta': delta, 'pct_change': pct_delta\n",
    "        })\n",
    "\n",
    "df_delta = pd.DataFrame(delta_rows)\n",
    "print('=== Feature Parameter Changes: Run 32 → 33 ===')\n",
    "# Show only params that changed\n",
    "changed = df_delta[df_delta.delta.notna() & (df_delta.delta.abs() > 0)]\n",
    "display(changed.sort_values('pct_change', ascending=False, key=abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation table: per param, per bet type\n",
    "rec_rows = []\n",
    "for _, row in df_boundary.iterrows():\n",
    "    if row['at_boundary'] == 'LOW':\n",
    "        action = f\"EXPAND lower bound below {row['lo']}\"\n",
    "    elif row['at_boundary'] == 'HIGH':\n",
    "        action = f\"EXPAND upper bound above {row['hi']}\"\n",
    "    else:\n",
    "        action = 'Keep current range'\n",
    "    rec_rows.append({\n",
    "        'run': row['run'], 'bet_type': row['bet_type'], 'param': row['param'],\n",
    "        'value': row['value'], 'range': f\"[{row['lo']}, {row['hi']}]\",\n",
    "        'position': f\"{row['pct_of_range']:.0%}\",\n",
    "        'recommendation': action\n",
    "    })\n",
    "\n",
    "df_rec = pd.DataFrame(rec_rows)\n",
    "# Show run 33 recommendations (most recent)\n",
    "print('=== Run 33 Parameter Range Recommendations ===')\n",
    "display(df_rec[df_rec.run == 33].sort_values(['param', 'bet_type'])\n",
    "        [['bet_type', 'param', 'value', 'range', 'position', 'recommendation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Hyperparameter Tuning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model comparison: run 32 vs 33\n",
    "model_comp_rows = []\n",
    "for bt in BET_TYPES:\n",
    "    for run_id in [32, 33]:\n",
    "        s = sniper[run_id].get(bt)\n",
    "        if s is None:\n",
    "            continue\n",
    "        wf = s.get('walkforward', {})\n",
    "        wf_best = wf.get('best_model_wf', s.get('best_model'))\n",
    "        model_comp_rows.append({\n",
    "            'bet_type': bt, 'run': run_id,\n",
    "            'best_model': s.get('best_model'),\n",
    "            'wf_best_model': wf_best,\n",
    "            'threshold': s.get('best_threshold'),\n",
    "            'bt_roi': s.get('roi'),\n",
    "            'holdout_roi': s.get('holdout_metrics', {}).get('roi'),\n",
    "        })\n",
    "\n",
    "df_models = pd.DataFrame(model_comp_rows)\n",
    "print('=== Best Model per Bet Type ===')\n",
    "pivot = df_models.pivot(index='bet_type', columns='run', values=['best_model', 'wf_best_model', 'holdout_roi'])\n",
    "display(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter comparison: extract best_params from sniper JSONs\n",
    "hp_rows = []\n",
    "for bt in BET_TYPES:\n",
    "    for run_id in [32, 33]:\n",
    "        s = sniper[run_id].get(bt)\n",
    "        if s is None:\n",
    "            continue\n",
    "        bp = s.get('best_params', {})\n",
    "        row = {'bet_type': bt, 'run': run_id, 'model': s.get('best_model')}\n",
    "        row.update(bp)\n",
    "        hp_rows.append(row)\n",
    "\n",
    "df_hp = pd.DataFrame(hp_rows)\n",
    "print('=== Model Hyperparameters ===')\n",
    "display(df_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-algorithm walkforward ROI: bar chart\n",
    "wf_rows = []\n",
    "for run_id in [32, 33]:\n",
    "    for bt in BET_TYPES:\n",
    "        s = sniper[run_id].get(bt)\n",
    "        if s is None:\n",
    "            continue\n",
    "        wf_summary = s.get('walkforward', {}).get('summary', {})\n",
    "        for model_name, metrics in wf_summary.items():\n",
    "            wf_rows.append({\n",
    "                'run': run_id, 'bet_type': bt, 'model': model_name,\n",
    "                'avg_roi': metrics.get('avg_roi', 0),\n",
    "                'std_roi': metrics.get('std_roi', 0),\n",
    "                'avg_precision': metrics.get('avg_precision', 0),\n",
    "                'total_bets': metrics.get('total_bets', 0),\n",
    "            })\n",
    "\n",
    "df_wf = pd.DataFrame(wf_rows)\n",
    "\n",
    "# Plot per-algorithm average ROI for run 33\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "for ax, bt in zip(axes.flat, BET_TYPES):\n",
    "    sub = df_wf[(df_wf.bet_type == bt)].copy()\n",
    "    if sub.empty:\n",
    "        ax.set_title(f'{bt} (no data)')\n",
    "        continue\n",
    "    pivot_data = sub.pivot_table(index='model', columns='run', values='avg_roi', aggfunc='first')\n",
    "    pivot_data.plot(kind='bar', ax=ax, width=0.7)\n",
    "    ax.axhline(0, color='black', lw=0.5)\n",
    "    ax.set_title(bt, fontsize=11)\n",
    "    ax.set_ylabel('Avg ROI %')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend(title='Run', fontsize=7)\n",
    "\n",
    "fig.suptitle('Walk-Forward Average ROI per Algorithm per Bet Type', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble vs single model analysis\n",
    "single_models = ['lightgbm', 'catboost', 'xgboost']\n",
    "ensemble_models = ['stacking', 'average', 'agreement']\n",
    "\n",
    "ens_rows = []\n",
    "for run_id in [32, 33]:\n",
    "    for bt in BET_TYPES:\n",
    "        sub = df_wf[(df_wf.run == run_id) & (df_wf.bet_type == bt)]\n",
    "        singles = sub[sub.model.isin(single_models)]\n",
    "        ensembles = sub[sub.model.isin(ensemble_models)]\n",
    "        if singles.empty or ensembles.empty:\n",
    "            continue\n",
    "        best_single = singles.loc[singles.avg_roi.idxmax()]\n",
    "        best_ens = ensembles.loc[ensembles.avg_roi.idxmax()]\n",
    "        ens_rows.append({\n",
    "            'run': run_id, 'bet_type': bt,\n",
    "            'best_single': best_single['model'],\n",
    "            'single_roi': best_single['avg_roi'],\n",
    "            'best_ensemble': best_ens['model'],\n",
    "            'ensemble_roi': best_ens['avg_roi'],\n",
    "            'ensemble_wins': best_ens['avg_roi'] > best_single['avg_roi'],\n",
    "            'roi_delta': best_ens['avg_roi'] - best_single['avg_roi'],\n",
    "        })\n",
    "\n",
    "df_ens = pd.DataFrame(ens_rows)\n",
    "print('=== Ensemble vs Single Model (Run 33) ===')\n",
    "display(df_ens[df_ens.run == 33].sort_values('roi_delta', ascending=False))\n",
    "print(f\"\\nEnsemble wins in {df_ens[df_ens.run == 33].ensemble_wins.sum()}/{len(df_ens[df_ens.run == 33])} bet types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SHAP & Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top features per bet type per run\n",
    "top_features = {}  # top_features[run][bt] = list of (feature, importance)\n",
    "for run_id in [32, 33]:\n",
    "    top_features[run_id] = {}\n",
    "    for bt in BET_TYPES:\n",
    "        s = sniper[run_id].get(bt)\n",
    "        if s is None:\n",
    "            continue\n",
    "        shap = s.get('shap_analysis', {})\n",
    "        tf = shap.get('top_features', [])\n",
    "        top_features[run_id][bt] = [(f['feature'], f['importance']) for f in tf[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side top 20 feature importance bar charts\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "for ax, bt in zip(axes.flat, BET_TYPES):\n",
    "    tf32 = dict(top_features[32].get(bt, []))\n",
    "    tf33 = dict(top_features[33].get(bt, []))\n",
    "    all_feats = list(dict.fromkeys(list(tf33.keys())[:15] + list(tf32.keys())[:15]))[:15]\n",
    "    if not all_feats:\n",
    "        ax.set_title(f'{bt} (no SHAP data)')\n",
    "        continue\n",
    "    y = np.arange(len(all_feats))\n",
    "    vals32 = [tf32.get(f, 0) for f in all_feats]\n",
    "    vals33 = [tf33.get(f, 0) for f in all_feats]\n",
    "    ax.barh(y + 0.15, vals33, height=0.3, label='R33', color='C1', alpha=0.8)\n",
    "    ax.barh(y - 0.15, vals32, height=0.3, label='R32', color='C0', alpha=0.8)\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels([f[:25] for f in all_feats], fontsize=7)\n",
    "    ax.set_title(bt, fontsize=11)\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "fig.suptitle('Top 15 Feature Importances (SHAP) per Bet Type', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature stability: rank correlation between runs\n",
    "stability_rows = []\n",
    "for bt in BET_TYPES:\n",
    "    tf32 = top_features[32].get(bt, [])\n",
    "    tf33 = top_features[33].get(bt, [])\n",
    "    if len(tf32) < 5 or len(tf33) < 5:\n",
    "        continue\n",
    "    feats32 = [f for f, _ in tf32]\n",
    "    feats33 = [f for f, _ in tf33]\n",
    "    common = set(feats32[:20]) & set(feats33[:20])\n",
    "    \n",
    "    # Spearman on common features\n",
    "    if len(common) >= 3:\n",
    "        ranks32 = {f: i for i, f in enumerate(feats32)}\n",
    "        ranks33 = {f: i for i, f in enumerate(feats33)}\n",
    "        common_list = sorted(common)\n",
    "        r32 = [ranks32[f] for f in common_list]\n",
    "        r33 = [ranks33[f] for f in common_list]\n",
    "        rho, pval = stats.spearmanr(r32, r33)\n",
    "    else:\n",
    "        rho, pval = np.nan, np.nan\n",
    "    \n",
    "    stability_rows.append({\n",
    "        'bet_type': bt,\n",
    "        'top20_overlap': len(common),\n",
    "        'overlap_pct': len(common) / 20 * 100,\n",
    "        'spearman_rho': rho,\n",
    "        'p_value': pval,\n",
    "    })\n",
    "\n",
    "df_stability = pd.DataFrame(stability_rows)\n",
    "print('=== Feature Ranking Stability (Run 32 vs 33) ===')\n",
    "display(df_stability.sort_values('spearman_rho', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-bet-type feature overlap heatmap (run 33)\n",
    "all_top20 = {}\n",
    "for bt in BET_TYPES:\n",
    "    tf = top_features[33].get(bt, [])\n",
    "    all_top20[bt] = set(f for f, _ in tf[:20])\n",
    "\n",
    "overlap_matrix = pd.DataFrame(index=BET_TYPES, columns=BET_TYPES, dtype=float)\n",
    "for bt1 in BET_TYPES:\n",
    "    for bt2 in BET_TYPES:\n",
    "        s1, s2 = all_top20.get(bt1, set()), all_top20.get(bt2, set())\n",
    "        if s1 and s2:\n",
    "            overlap_matrix.loc[bt1, bt2] = len(s1 & s2)\n",
    "        else:\n",
    "            overlap_matrix.loc[bt1, bt2] = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(overlap_matrix.astype(float), annot=True, fmt='.0f', cmap='YlGn',\n",
    "            ax=ax, vmin=0, vmax=20, linewidths=0.5)\n",
    "ax.set_title('Top-20 Feature Overlap Between Bet Types (Run 33)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low importance features: candidates for removal\n",
    "low_imp_union = set()\n",
    "low_imp_counts = defaultdict(int)  # feature -> count of bet types where it's low importance\n",
    "\n",
    "for run_id in [33]:  # Focus on latest run\n",
    "    for bt in BET_TYPES:\n",
    "        s = sniper[run_id].get(bt)\n",
    "        if s is None:\n",
    "            continue\n",
    "        shap = s.get('shap_analysis', {})\n",
    "        low = shap.get('low_importance_features', [])\n",
    "        for f in low:\n",
    "            low_imp_counts[f] += 1\n",
    "            low_imp_union.add(f)\n",
    "\n",
    "# Features that are low importance across many bet types\n",
    "print(f'Total unique low-importance features: {len(low_imp_union)}')\n",
    "print(f'\\n=== Features with low importance in 7+ bet types (safe removal candidates) ===')\n",
    "removal_candidates = {f: c for f, c in low_imp_counts.items() if c >= 7}\n",
    "for f, c in sorted(removal_candidates.items(), key=lambda x: -x[1]):\n",
    "    print(f'  {f}: low in {c}/9 bet types')\n",
    "\n",
    "print(f'\\n=== Features with low importance in 5-6 bet types (review candidates) ===')\n",
    "review_candidates = {f: c for f, c in low_imp_counts.items() if 5 <= c <= 6}\n",
    "for f, c in sorted(review_candidates.items(), key=lambda x: -x[1]):\n",
    "    print(f'  {f}: low in {c}/9 bet types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature interactions: top 10 per bet type\n",
    "interaction_rows = []\n",
    "for run_id in [33]:\n",
    "    for bt in BET_TYPES:\n",
    "        s = sniper[run_id].get(bt)\n",
    "        if s is None:\n",
    "            continue\n",
    "        shap = s.get('shap_analysis', {})\n",
    "        interactions = shap.get('feature_interactions', [])\n",
    "        for ix in interactions[:10]:\n",
    "            interaction_rows.append({\n",
    "                'bet_type': bt,\n",
    "                'feature1': ix.get('feature1', ''),\n",
    "                'feature2': ix.get('feature2', ''),\n",
    "                'strength': ix.get('interaction_strength', 0),\n",
    "            })\n",
    "\n",
    "df_interactions = pd.DataFrame(interaction_rows)\n",
    "if not df_interactions.empty:\n",
    "    # Cross-bet-type interaction patterns\n",
    "    df_interactions['pair'] = df_interactions.apply(\n",
    "        lambda r: tuple(sorted([r['feature1'], r['feature2']])), axis=1\n",
    "    )\n",
    "    pair_counts = df_interactions.groupby('pair').agg(\n",
    "        n_bet_types=('bet_type', 'nunique'),\n",
    "        avg_strength=('strength', 'mean'),\n",
    "        bet_types=('bet_type', lambda x: ', '.join(sorted(set(x))))\n",
    "    ).sort_values('n_bet_types', ascending=False)\n",
    "\n",
    "    print('=== Feature Interactions Appearing in Multiple Bet Types ===')\n",
    "    display(pair_counts[pair_counts.n_bet_types >= 2].head(15))\n",
    "else:\n",
    "    print('No interaction data available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Business Metrics & Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison table\n",
    "print('=== Precision / ROI / Sharpe / Sortino: Run 32 vs 33 ===')\n",
    "metrics_cols = ['bet_type', 'run', 'best_model', 'bt_precision', 'bt_roi', 'bt_n_bets',\n",
    "                'holdout_precision', 'holdout_roi', 'holdout_n_bets',\n",
    "                'holdout_sharpe', 'holdout_sortino', 'holdout_ece']\n",
    "display(df_sniper[metrics_cols].sort_values(['bet_type', 'run']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Overfitting gap: backtest ROI vs holdout ROI\ndf_sniper['overfit_gap'] = df_sniper['bt_roi'] - df_sniper['holdout_roi']\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\nfor ax, run_id in zip(axes, [32, 33]):\n    sub = df_sniper[df_sniper.run == run_id].copy().reset_index(drop=True)\n    x = np.arange(len(sub))\n    w = 0.35\n    ax.bar(x - w/2, sub['bt_roi'], w, label='Backtest ROI', color='C0', alpha=0.8)\n    ax.bar(x + w/2, sub['holdout_roi'].fillna(0), w, label='Holdout ROI', color='C3', alpha=0.8)\n    ax.set_xticks(x)\n    ax.set_xticklabels(sub['bet_type'], rotation=45, ha='right')\n    ax.axhline(0, color='black', lw=0.5)\n    ax.set_title(f'Run {run_id}: Backtest vs Holdout ROI')\n    ax.set_ylabel('ROI %')\n    ax.legend()\n    # Annotate large gaps\n    for i, row in sub.iterrows():\n        gap = row['overfit_gap']\n        if pd.notna(gap) and abs(gap) > 5:\n            y_pos = max(row['bt_roi'], row.get('holdout_roi', 0) or 0) + 2\n            color = 'red' if gap > 20 else 'orange'\n            ax.text(i, y_pos, f'gap={gap:.0f}%', ha='center', fontsize=7, color=color)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward stability: std of ROI across folds\n",
    "wf_stability_rows = []\n",
    "for run_id in [32, 33]:\n",
    "    for bt in BET_TYPES:\n",
    "        s = sniper[run_id].get(bt)\n",
    "        if s is None:\n",
    "            continue\n",
    "        wf = s.get('walkforward', {}).get('summary', {})\n",
    "        best_model = s.get('best_model', '')\n",
    "        metrics = wf.get(best_model, {})\n",
    "        avg_roi = metrics.get('avg_roi', 0)\n",
    "        std_roi = metrics.get('std_roi', 0)\n",
    "        cv = std_roi / (abs(avg_roi) + 1e-9) if avg_roi != 0 else np.inf\n",
    "        wf_stability_rows.append({\n",
    "            'run': run_id, 'bet_type': bt, 'model': best_model,\n",
    "            'avg_roi': avg_roi, 'std_roi': std_roi, 'cv_coeff': cv,\n",
    "            'total_bets': metrics.get('total_bets', 0),\n",
    "        })\n",
    "\n",
    "df_wf_stability = pd.DataFrame(wf_stability_rows)\n",
    "print('=== Walk-Forward Stability (best model per bet type) ===')\n",
    "display(df_wf_stability.sort_values(['bet_type', 'run']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bet volume vs quality tradeoff\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nfor ax, run_id in zip(axes, [32, 33]):\n    sub = df_sniper[df_sniper.run == run_id].dropna(subset=['holdout_n_bets', 'holdout_roi'])\n    ax.scatter(sub['holdout_n_bets'], sub['holdout_roi'], s=100, zorder=3)\n    for _, row in sub.iterrows():\n        ax.text(row['holdout_n_bets'] + 2, row['holdout_roi'] + 2, row['bet_type'], fontsize=8)\n    ax.axhline(0, color='red', ls='--', alpha=0.5)\n    ax.set_xlabel('Number of Bets (Holdout)')\n    ax.set_ylabel('Holdout ROI %')\n    ax.set_title(f'Run {run_id}: Volume vs Quality')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECE calibration analysis\n",
    "print('=== Expected Calibration Error (lower is better, <0.05 = well calibrated) ===')\n",
    "ece_data = df_sniper[['bet_type', 'run', 'holdout_ece']].pivot(\n",
    "    index='bet_type', columns='run', values='holdout_ece'\n",
    ")\n",
    "ece_data['delta'] = ece_data[33] - ece_data[32]\n",
    "ece_data['well_calibrated_33'] = ece_data[33] < 0.05\n",
    "display(ece_data.sort_values(33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Portfolio-level analysis\nprint('=== Portfolio Analysis (All Bet Types Combined) ===')\nfor run_id in [32, 33]:\n    sub = df_sniper[df_sniper.run == run_id].dropna(subset=['holdout_n_bets', 'holdout_roi'])\n    total_bets = sub['holdout_n_bets'].sum()\n    weighted_roi = (sub['holdout_roi'] * sub['holdout_n_bets']).sum() / total_bets\n    profitable = sub[sub.holdout_roi > 0]\n    print(f'\\nRun {run_id}:')\n    print(f'  Total holdout bets: {total_bets:.0f}')\n    print(f'  Weighted avg holdout ROI: {weighted_roi:.1f}%')\n    print(f'  Profitable markets: {len(profitable)}/{len(sub)}')\n    print(f'  Best market: {sub.loc[sub.holdout_roi.idxmax(), \"bet_type\"]} ({sub.holdout_roi.max():.1f}%)')\n    print(f'  Worst market: {sub.loc[sub.holdout_roi.idxmin(), \"bet_type\"]} ({sub.holdout_roi.min():.1f}%)')\n\n# Correlation between runs\nprint('\\n=== Holdout ROI Correlation Between Runs ===')\nroi_pivot = df_sniper.pivot(index='bet_type', columns='run', values='holdout_roi').dropna()\nif len(roi_pivot) >= 3 and roi_pivot.shape[1] == 2:\n    corr, pval = stats.pearsonr(roi_pivot[32], roi_pivot[33])\n    print(f'Pearson correlation of holdout ROI (R32 vs R33): {corr:.3f} (p={pval:.3f})')\n    print(f'Based on {len(roi_pivot)} bet types with holdout data in both runs')\nelse:\n    print('Insufficient overlapping data for correlation')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/disable recommendation\n",
    "print('=== Market Enable/Disable Recommendations (Based on Run 33 Holdout) ===')\n",
    "rec_rows = []\n",
    "for bt in BET_TYPES:\n",
    "    s33 = sniper[33].get(bt)\n",
    "    s32 = sniper[32].get(bt)\n",
    "    if s33 is None:\n",
    "        continue\n",
    "    h33 = s33.get('holdout_metrics', {})\n",
    "    h32 = s32.get('holdout_metrics', {}) if s32 else {}\n",
    "    roi33 = h33.get('roi', 0)\n",
    "    prec33 = h33.get('precision', 0)\n",
    "    roi32 = h32.get('roi', 0)\n",
    "    \n",
    "    if roi33 < 0 and roi32 < 0:\n",
    "        status = 'DISABLE'\n",
    "        reason = f'Negative ROI in both runs ({roi32:.1f}%, {roi33:.1f}%)'\n",
    "    elif roi33 < 0:\n",
    "        status = 'REVIEW'\n",
    "        reason = f'Negative holdout ROI in R33 ({roi33:.1f}%), positive in R32 ({roi32:.1f}%)'\n",
    "    elif prec33 < 0.55:\n",
    "        status = 'CAUTION'\n",
    "        reason = f'Precision below 55% ({prec33:.1%})'\n",
    "    else:\n",
    "        status = 'ENABLE'\n",
    "        reason = f'ROI={roi33:.1f}%, Precision={prec33:.1%}'\n",
    "    \n",
    "    rec_rows.append({\n",
    "        'bet_type': bt, 'status': status, 'reason': reason,\n",
    "        'holdout_roi_r32': roi32, 'holdout_roi_r33': roi33,\n",
    "        'holdout_precision_r33': prec33,\n",
    "    })\n",
    "\n",
    "df_market_rec = pd.DataFrame(rec_rows)\n",
    "display(df_market_rec.sort_values('holdout_roi_r33', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Actionable Recommendations Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('ACTIONABLE RECOMMENDATIONS FOR RUN 34')\n",
    "print('=' * 80)\n",
    "\n",
    "# 1. Feature param recommendations\n",
    "print('\\n--- FEATURE PARAMETER RANGE CHANGES ---')\n",
    "flagged_33 = df_boundary[(df_boundary.run == 33) & (df_boundary.at_boundary != 'OK')]\n",
    "if len(flagged_33) > 0:\n",
    "    for _, row in flagged_33.iterrows():\n",
    "        direction = 'lower' if row['at_boundary'] == 'LOW' else 'upper'\n",
    "        current_bound = row['lo'] if direction == 'lower' else row['hi']\n",
    "        suggested = int(current_bound * 0.7) if direction == 'lower' else int(current_bound * 1.4)\n",
    "        print(f\"  {row['bet_type']}/{row['param']}: Expand {direction} bound \"\n",
    "              f\"from {current_bound} to ~{suggested} (value={row['value']})\")\n",
    "else:\n",
    "    print('  No parameters at boundary in run 33 - search spaces are adequate.')\n",
    "\n",
    "# 2. Model config\n",
    "print('\\n--- MODEL SELECTION PER BET TYPE ---')\n",
    "for bt in BET_TYPES:\n",
    "    s33 = sniper[33].get(bt)\n",
    "    if s33 is None:\n",
    "        continue\n",
    "    model = s33.get('best_model', 'unknown')\n",
    "    wf_best = s33.get('walkforward', {}).get('best_model_wf', model)\n",
    "    match = 'CONSISTENT' if model == wf_best else f'MISMATCH (selected={model}, wf_best={wf_best})'\n",
    "    print(f'  {bt}: {model} [{match}]')\n",
    "\n",
    "# 3. Strategy decisions\n",
    "print('\\n--- MARKET STATUS RECOMMENDATIONS ---')\n",
    "for _, row in df_market_rec.sort_values('holdout_roi_r33', ascending=False).iterrows():\n",
    "    icon = {'ENABLE': '+', 'CAUTION': '~', 'REVIEW': '?', 'DISABLE': '-'}[row['status']]\n",
    "    print(f\"  [{icon}] {row['bet_type']}: {row['status']} - {row['reason']}\")\n",
    "\n",
    "# 4. Run 34 config suggestions\n",
    "print('\\n--- RUN 34 CONFIGURATION ---')\n",
    "print('  Model trials: Keep at 75 (diminishing returns above this based on R32 vs R33 comparison)')\n",
    "print('  Feature trials: Keep at 18 (convergence observed in most bet types)')\n",
    "\n",
    "# Check if more trials helped\n",
    "r32_avg_holdout = df_sniper[df_sniper.run == 32].holdout_roi.mean()\n",
    "r33_avg_holdout = df_sniper[df_sniper.run == 33].holdout_roi.mean()\n",
    "print(f'  R32 avg holdout ROI (115 trials): {r32_avg_holdout:.1f}%')\n",
    "print(f'  R33 avg holdout ROI (75 trials): {r33_avg_holdout:.1f}%')\n",
    "if r33_avg_holdout >= r32_avg_holdout:\n",
    "    print('  -> 75 trials performed equal or better. No need to increase.')\n",
    "else:\n",
    "    diff = r32_avg_holdout - r33_avg_holdout\n",
    "    print(f'  -> R32 outperformed by {diff:.1f}pp. Consider increasing trials back to 100+.')\n",
    "\n",
    "# Feature removal suggestions\n",
    "n_safe_removal = sum(1 for c in low_imp_counts.values() if c >= 7)\n",
    "print(f'\\n  Feature removal: {n_safe_removal} features are low-importance in 7+ bet types.')\n",
    "print('  Consider removing these to reduce dimensionality and speed up training.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}