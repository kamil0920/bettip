{
  "version": "3.2",
  "last_updated": "2026-02-18",
  "project_overview": {
    "_guidance": "High-level what and why. Keep to 2-3 sentences max. No technical details here.",
    "name": "bettip",
    "description": "ML-powered sports betting system that identifies value bets across 10 European football leagues using walk-forward validated GBDT models, calibrated probabilities, and agreement ensembles. Targets niche stat markets (fouls, cards, corners, shots over/under lines) where bookmaker inefficiencies are exploitable.",
    "platform": "Python CLI + GitHub Actions CI + HuggingFace Hub + Telegram",
    "primary_technology": "Python 3.10, scikit-learn, XGBoost, LightGBM, CatBoost, pandas, Optuna",
    "project_type": "ML pipeline (data collection → features → training → calibration → inference → recommendations)"
  },
  "domain_context": {
    "_guidance": "Technical environment only — frameworks, patterns, and gotchas specific to the domain. No project goals or architecture here.",
    "frameworks": [
      "scikit-learn 1.7+ (ML pipeline, CalibratedClassifierCV, RFECV, TimeSeriesSplit)",
      "XGBoost, LightGBM, CatBoost (gradient-boosted decision tree models)",
      "Optuna (Bayesian hyperparameter optimization, 150 trials per market)",
      "pandas 2.x / numpy (data processing, parquet I/O)",
      "FastAI tabular (optional deep learning, entity embeddings)",
      "MAPIE (conformal prediction, uncertainty quantification)",
      "MLflow (experiment tracking)",
      "Metaflow (DAG pipeline orchestration)",
      "HuggingFace Hub (private dataset repo for data/models/configs)",
      "API-Football v3 (match data, lineups, events, fixtures)",
      "football-data.co.uk (historical bookmaker odds for H2H markets)",
      "The Odds API v4 (real-time odds, quota-limited)"
    ],
    "high_level_design_patterns": [
      "Walk-forward cross-validation with feature-aware embargo (never random splits)",
      "Two-stage models: Stage 1 probability estimation + Stage 2 edge estimation",
      "Agreement ensembles: multiple models must independently agree above threshold",
      "DisagreementEnsemble: conservative/balanced/aggressive presets for ensemble variance",
      "Per-market optimization: each betting market has independent model, features, thresholds, calibration",
      "Registry pattern for feature engineers (42 registered, 16 engineer classes)",
      "Strategy pattern for betting logic and model selection",
      "Factory pattern for model creation (6 model types)",
      "Selective regeneration with hash-based caching for feature engineering"
    ],
    "technical_notes": [
      "CRITICAL: Data leakage is the #1 recurring bug. _x/_y column collisions after merges, cross-market feature contamination, and temporal ordering violations have caused multiple false-positive results.",
      "Walk-forward CV uses feature-aware dynamic embargo (94-217 days by market) computed from feature config lookback windows.",
      "ECE (Expected Calibration Error) drift is the #1 predictor of live market failure. ECE > 0.10 = do not deploy.",
      "Seed sensitivity was 88-103pp ROI gap until S27 measurement hardening (embargo + mRMR) reduced it to 13pp average.",
      "UNDER line variants dominate performance. Agreement ensembles are the winning architecture.",
      "H2H markets (home_win, away_win, over25, btts) have consistently failed since S16 — no profitable deployment found.",
      "Niche markets use fallback (fake) odds at inference time — no historical bulk odds data for fouls/corners/cards/shots lines.",
      "CatBoost requires has_time=True for temporal ordering. CalibratedClassifierCV must use TimeSeriesSplit not StratifiedKFold.",
      "Max 5 bet types per CI workflow dispatch to avoid HuggingFace Hub 429 rate limits from parallel matrix jobs.",
      "CatBoost monotonic constraints crash with Depthwise grow_policy — guard on SymmetricTree only.",
      "CatBoost transfer learning ft_iterations param must be stripped from best_params before constructor call."
    ]
  },
  "technical_architecture": {
    "_guidance": "How YOUR system is structured. Components, relationships, and file organization.",
    "architecture_summary": "Pipeline architecture: API-Football → raw parquet → preprocessed → features (608 cols, 19,075 rows across 10 leagues) → ML training (nested walk-forward CV, RFECV feature selection, Optuna tuning) → calibrated predictions → daily recommendations. 24 betting market configs (15 enabled). Models and data stored on HuggingFace Hub, optimization via GitHub Actions CI, daily predictions via scheduled workflows, notifications via Telegram.",
    "major_components": [
      "src/data_collection/ — API-Football clients, match collectors, weather collector",
      "src/preprocessing/ — Raw data parsers, validators, stat extractors",
      "src/features/engineers/ — 16 modular feature engineer classes (ELO, Poisson, form, H2H, stats, niche, context, lineup, corners, cross-market, referee, etc.)",
      "src/features/regeneration.py (1,026 lines) — Selective feature regeneration with hash-based caching, two-pass architecture, leakage column stripping",
      "src/features/config_manager.py (470 lines) — Per-bet-type feature parameter management and Optuna search spaces",
      "src/ml/models.py (350 lines) — Model factory for RF, XGB, LGB, CatBoost, LR, FastAI",
      "src/ml/catboost_wrapper.py (121 lines) — EnhancedCatBoost with transfer learning, baseline injection, monotonic constraints",
      "src/ml/two_stage_model.py (457 lines) — Two-stage probability + edge estimation model",
      "src/ml/ensemble.py — Stacking ensemble with LogisticRegression meta-learner",
      "src/ml/ensemble_disagreement.py — DisagreementEnsemble (conservative/balanced/aggressive)",
      "src/calibration/calibration.py (644 lines) — Sigmoid, isotonic, beta, temperature, Venn-Abers calibration",
      "src/odds/odds_merger.py (274 lines) — Merges football-data.co.uk bookmaker odds into features",
      "src/pipelines/betting_training_pipeline.py (1,073 lines) — Training orchestration with nested CV, RFECV, Optuna",
      "experiments/run_sniper_optimization.py (4,755 lines) — Per-market production optimization with 48 CLI flags",
      "experiments/generate_daily_recommendations.py (1,392 lines) — Daily prediction pipeline with lineup injection",
      "config/sniper_deployment.json — Auto-generated deployment config mapping markets to models/thresholds/features"
    ],
    "file_structure_overview": [
      "src/ — Core library code (data_collection, preprocessing, features, ml, calibration, odds, pipelines, recommendations, monitoring, paper_trading)",
      "entrypoints/ — CLI entry points (download_data, collect, preprocess, features, run_pipeline, daily_pipeline, paper_trade, upload_data)",
      "experiments/ — Optimization scripts, daily recommendations, analysis (13 scripts)",
      "scripts/ — Data collection utilities (regenerate_all_features, collect_coach_data, collect_expansion_match_stats, backfill_coach_data)",
      "flows/ — Metaflow DAG definitions (betting_flow, daily_inference_flow)",
      "config/ — Per-league YAMLs (10 active), strategies.yaml, training_config.yaml, sniper_deployment.json, feature_params/*.yaml",
      "data/ — Pipeline stages: 01-raw, 02-preprocessed, 03-features, 04-predictions, 05-recommendations, 06-prematch, 07-injuries, odds_cache",
      "models/ — Production model files (.joblib)",
      "tests/ — 883 tests across 44 files (39 unit, 3 integration, 1 data leakage, 1 conftest)",
      ".github/workflows/ — sniper-optimization.yaml, prematch-intelligence.yaml, collect-match-data.yaml"
    ]
  },
  "project_invariants": {
    "_guidance": "Anchors that prevent AI drift over long sessions.",
    "architecture_style": "Pipeline architecture with modular feature engineers, per-market model optimization, and walk-forward temporal validation",
    "architecture_drift_rule": "No introduction of new architectural patterns without explicit decision_log entry and human approval.",
    "non_negotiable_patterns": [
      "Walk-forward cross-validation only — never random shuffle for time series match data",
      "Feature-aware dynamic embargo between train/test splits (computed from feature config lookback windows)",
      "Calibrated probabilities before any betting decisions — ECE < 0.10 required for deployment",
      "Data leakage prevention: no future information in features, check for _x/_y columns after every merge",
      "Per-market independence: each market has its own model, features, thresholds, calibration method",
      "Agreement ensembles require multiple models above threshold before recommending a bet",
      "Nested CV for hyperparameter tuning to prevent optimistic bias",
      "Minimum 20 holdout bets before deploying any market",
      "Seed robustness check (<30pp gap between seed=42 and seed=123) before deployment",
      "TimeSeriesSplit for CalibratedClassifierCV — never StratifiedKFold with shuffle",
      "Run pytest before committing — all 883 tests must pass"
    ],
    "forbidden_patterns": [
      "Random shuffle of time series data (StratifiedKFold with shuffle=True)",
      "Using future data in features (data leakage)",
      "Deploying models with ECE > 0.10",
      "Deploying markets with fewer than 20 holdout bets",
      "Hardcoded/fake odds for edge calculation in H2H markets",
      "Empty catch blocks that silently swallow errors",
      "Adding new pip dependencies without explicit approval",
      "Bare except clauses",
      "Modifying config/sniper_deployment.json without analyzing CI optimization results first",
      "Using pd.merge without checking for _x/_y suffix columns afterward",
      "Passing CatBoost transfer learning params (ft_iterations) to constructor — must strip first"
    ],
    "terminology": {
      "_guidance": "Define canonical terms so the AI never renames or drifts on project vocabulary.",
      "walk-forward CV": "Time-series cross-validation where training window grows and test window slides forward. Never random split.",
      "embargo": "Calendar-day gap between train and test sets to prevent feature lookback data overlap. Computed dynamically from feature config.",
      "holdout": "Last N folds of walk-forward CV reserved for final evaluation. Never used during model selection or tuning.",
      "agreement ensemble": "Strategy requiring multiple independent models to predict above threshold before generating a recommendation.",
      "two-stage model": "Stage 1 estimates probability (will bet land?). Stage 2 estimates edge (how much value?). Combined for final recommendation.",
      "ECE": "Expected Calibration Error. Measures predicted probability vs actual outcome frequency. <0.05 excellent, <0.10 deployable, >0.10 reject.",
      "CLV": "Closing Line Value. Did the model beat the closing line? The true measure of a betting edge.",
      "edge": "Model probability minus implied probability from bookmaker odds. Positive edge = value bet.",
      "niche markets": "Non-standard betting markets: fouls, cards, corners, shots at specific over/under lines (e.g., fouls_under_255).",
      "H2H markets": "Standard match outcome markets: home_win, away_win, over 2.5 goals, under 2.5 goals, BTTS.",
      "sniper deployment": "Production config (config/sniper_deployment.json) mapping each market to its optimized model, features, and thresholds.",
      "mRMR": "Minimum Redundancy Maximum Relevance. Post-RFECV feature reranking that minimizes inter-feature correlation while maximizing target correlation.",
      "RFECV": "Recursive Feature Elimination with Cross-Validation. Automated feature selection that iteratively removes least important features.",
      "baseline odds": "Fallback odds used when real bookmaker odds unavailable. Not reliable for edge calculation — produces fake edges.",
      "measurement hardening": "S27 improvements: dynamic embargo, mRMR, aggressive regularization, KS tests. Reduced seed sensitivity from 88pp to 13pp.",
      "feature-param optimization": "Tuning feature engineering hyperparameters (EMA spans, ELO k-factors). Confirmed useless across 3 campaigns — do not use."
    }
  },
  "coding_conventions": {
    "naming": {
      "files": "snake_case.py (e.g., two_stage_model.py, odds_merger.py)",
      "functions": "snake_case (e.g., compute_embargo_days, get_cv_splits)",
      "variables": "snake_case (e.g., holdout_roi, n_bets)",
      "constants": "UPPER_SNAKE_CASE (e.g., EXCLUDE_COLUMNS, BET_TYPES, _CATBOOST_STRIP_KEYS)",
      "examples": [
        "src/ml/two_stage_model.py — file naming",
        "def compute_embargo_days(feature_config) — function naming",
        "holdout_roi = 0.45 — variable naming",
        "EXCLUDE_COLUMNS = ['goal_difference', 'total_goals'] — constant naming",
        "BET_TYPES = {'home_win': {...}} — dict constant naming"
      ]
    },
    "file_rules": {
      "max_lines_per_file": 500,
      "one_responsibility_per_file": true,
      "folder_structure_notes": "src/ organized by domain (data_collection, features, ml, calibration, odds, pipelines, recommendations). Each feature engineer is a separate file in src/features/engineers/. Config files in config/. Scripts in scripts/. Entrypoints in entrypoints/."
    },
    "function_rules": {
      "max_function_length": "50 lines preferred, 100 lines max for complex ML functions",
      "max_parameters_per_function": "8 (use config dataclasses or **kwargs for more)"
    },
    "style": {
      "indentation": "4 spaces",
      "quotes": "double quotes (enforced by black)",
      "trailing_commas": "yes (enforced by black)",
      "additional_rules": [
        "black for code formatting (line length 88)",
        "isort for import sorting (profile=black)",
        "Type hints required on all public functions",
        "Docstrings required on all public classes and functions",
        "No bare except clauses — always specify exception type",
        "Use logging module, not print statements",
        "Use pathlib.Path for file paths where practical"
      ]
    }
  },
  "dependency_policy": {
    "approved_dependencies": [
      "scikit-learn", "xgboost", "lightgbm", "catboost", "optuna",
      "pandas", "numpy", "scipy", "joblib",
      "fastai (optional, --extra dl)", "tabnet (optional)", "tabpfn (optional)",
      "mapie", "mlflow", "metaflow",
      "huggingface_hub", "requests", "python-dotenv",
      "black", "isort", "pytest", "pytest-cov",
      "pyarrow", "pyyaml", "python-telegram-bot"
    ],
    "approval_required_for_new": true,
    "prefer_built_in_over_third_party": true,
    "notes": "No new libraries without explicit human approval. This project already has a large dependency surface. Prefer stdlib, pandas, numpy, and scikit-learn built-ins before reaching for new packages."
  },
  "error_handling_standard": {
    "strategy": "Structured logging with context. Fail loudly in training, degrade gracefully in inference.",
    "strategy_examples": [
      "Training pipeline: raise on data leakage, missing targets, calibration failure",
      "Inference pipeline: log warnings for missing features/models, fill with median/0, skip market rather than crash",
      "API calls: retry with exponential backoff, track remaining quota, resume on failure"
    ],
    "required_patterns": [
      "Log errors with module name, function name, and input context",
      "Use structured warning categories: [MODEL MISMATCH], [FEATURE MISMATCH], [CALIBRATION COLLAPSE], [TWO STAGE], [NaN FILL], [LINE IMPLAUSIBLE]",
      "Validate DataFrame shapes and column presence after every merge operation",
      "Check for _x/_y suffix columns after pd.merge — fail if found",
      "Validate probability outputs: 0 <= p <= 1, no NaN, no inf"
    ],
    "forbidden_patterns": [
      "Empty catch blocks",
      "Generic error messages with no context",
      "Silently filling NaN without logging which features were affected",
      "Catching Exception broadly in training pipeline (let it crash to expose bugs)",
      "print() instead of logging"
    ]
  },
  "testing_strategy": {
    "approach": "unit-first, integration for pipeline paths, dedicated data leakage tests",
    "minimum_coverage_target": "All public functions in src/ must have unit tests. Critical paths (training, inference, calibration) must have integration tests.",
    "test_naming_convention": "test_{module}_{function_or_scenario} in tests/unit/test_{module}.py",
    "what_must_be_tested": [
      "All public functions in src/",
      "Data leakage prevention (tests/test_data_leakage.py)",
      "Edge cases identified by devil's advocate",
      "Error handling paths (missing features, model mismatches, calibration collapse)",
      "DataFrame shape and column integrity after merges",
      "Probability output validity (0-1 range, no NaN/inf)",
      "Walk-forward CV temporal ordering (train dates < test dates)",
      "Feature engineer output shapes and no-leakage guarantees"
    ],
    "what_can_skip_tests": [
      "CLI argument parsing boilerplate in entrypoints/",
      "Third-party API response mocking for rarely-called endpoints",
      "Visualization/plotting code"
    ]
  },
  "modules": [
    {
      "name": "live_odds_client",
      "file_path": "src/odds/live_odds_client.py",
      "status": "draft",
      "depends_on": ["src/odds/theodds_unified_loader.py"],
      "blocked_by": [],
      "estimated_complexity": "medium",
      "known_risks": [
        "The Odds API has strict rate limits and quota (currently 0 remaining)",
        "Niche market odds (fouls, corners, cards, shots lines) may not be available on The Odds API — need to verify market coverage",
        "Odds change rapidly pre-match — caching strategy must balance freshness vs quota usage"
      ],
      "acceptance_criteria": [
        "Fetch real-time odds for niche markets (fouls/corners/cards/shots over/under lines) for upcoming matches",
        "Graceful fallback when odds unavailable (return None, not fake odds)",
        "Quota tracking: log remaining API calls, abort if below safety threshold",
        "Cache layer: don't re-fetch odds that were retrieved within last 15 minutes",
        "Integration with generate_daily_recommendations.py — replace baseline odds with real odds when available",
        "Unit tests for caching, fallback, quota tracking",
        "No new dependencies — use existing requests library"
      ],
      "last_modified": "",
      "iteration_count": 0,
      "test_status": "",
      "owner_agent": "",
      "notes": "Critical gap: all niche market predictions currently use BASELINE (fake) odds, making edge calculations meaningless. This module provides real odds for inference."
    },
    {
      "name": "prediction_robustness",
      "file_path": "experiments/generate_daily_recommendations.py",
      "status": "draft",
      "depends_on": ["src/ml/model_loader.py", "src/odds/live_odds_client.py"],
      "blocked_by": [],
      "estimated_complexity": "medium",
      "known_risks": [
        "Feature mismatches between training and inference (4-5 features missing per model in production)",
        "Two-stage models silently skip when odds unavailable — no signal at all for those markets",
        "Calibration collapse (FastAI sigmoid degenerate) produces uncalibrated probabilities"
      ],
      "acceptance_criteria": [
        "Structured health report per prediction run: models loaded, features matched, calibration status, odds source",
        "Feature mismatch severity classification: <2% missing = proceed with warning, 2-5% = degrade confidence, >5% = skip market",
        "Two-stage model fallback: when odds unavailable, use Stage 1 probability-only prediction with reduced confidence flag",
        "Calibration collapse detection: if calibrator is degenerate, flag prediction as 'uncalibrated' and apply conservative threshold multiplier (1.2x)",
        "Model mismatch auto-resolution: if .joblib file missing, log error and skip market (don't crash entire pipeline)",
        "Summary JSON output with per-market status for debugging",
        "Unit tests for each degradation path"
      ],
      "last_modified": "",
      "iteration_count": 0,
      "test_status": "",
      "owner_agent": "",
      "notes": "Today's prediction run had 17 warnings across 7 categories. Pipeline produces predictions but quality is unknown when features are missing or calibration collapses."
    },
    {
      "name": "data_freshness_monitor",
      "file_path": "src/monitoring/data_freshness.py",
      "status": "draft",
      "depends_on": [],
      "blocked_by": [],
      "estimated_complexity": "low",
      "known_risks": [
        "API-Football quota could be exhausted by other collection scripts",
        "Feature regeneration takes 10-15 minutes — may miss kickoff if run too late"
      ],
      "acceptance_criteria": [
        "Check features parquet date range and warn if more than 7 days stale",
        "Compare today's fixture schedule (from API or local data) against features parquet — report matches missing from training data",
        "CLI command: python -m src.monitoring.data_freshness --check that returns exit code 0 (fresh) or 1 (stale) for CI integration",
        "Log which leagues have stale data and how many match-days are missing",
        "Integration with prematch-intelligence workflow: run freshness check before predictions, abort with clear message if data too stale",
        "Unit tests with mock parquet dates"
      ],
      "last_modified": "",
      "iteration_count": 0,
      "test_status": "",
      "owner_agent": "",
      "notes": "Features parquet ended Feb 3, causing Arsenal vs Wolves to be missed on Feb 18. Need early warning system."
    },
    {
      "name": "calibration_monitor",
      "file_path": "src/monitoring/calibration_monitor.py",
      "status": "draft",
      "depends_on": ["src/calibration/calibration.py", "src/ml/calibration_validator.py"],
      "blocked_by": [],
      "estimated_complexity": "medium",
      "known_risks": [
        "Detecting calibration collapse requires running calibrator on holdout data — needs access to training data splits",
        "ECE computation requires binning which is sensitive to bin count and sample size"
      ],
      "acceptance_criteria": [
        "Detect calibration collapse at inference time: identify degenerate sigmoid/isotonic fits before they produce uncalibrated probabilities",
        "Per-market ECE tracking over rolling window of last N predictions (configurable, default 50)",
        "ECE drift alert: warn when live ECE exceeds 2x training ECE for any market",
        "Reliability diagram generation for debugging (matplotlib, save to data/05-recommendations/diagnostics/)",
        "Integration with generate_daily_recommendations.py: add calibration health status to each prediction",
        "Unit tests for collapse detection, ECE computation, drift alerting"
      ],
      "last_modified": "",
      "iteration_count": 0,
      "test_status": "",
      "owner_agent": "",
      "notes": "Calibration collapse on corners_over_85 FastAI was detected only via log warning. Need proactive detection and clear flagging before predictions reach the user."
    },
    {
      "name": "bet_result_tracker",
      "file_path": "src/monitoring/bet_result_tracker.py",
      "status": "draft",
      "depends_on": ["src/data_collection/match_collector.py"],
      "blocked_by": [],
      "estimated_complexity": "medium",
      "known_risks": [
        "Match results may not be available immediately after the match — need polling/retry logic",
        "Niche market results (exact foul/corner/card counts) require detailed match stats, not just scorelines",
        "Historical weekend_log.md is on HF Hub — need to merge new results without overwriting"
      ],
      "acceptance_criteria": [
        "Given a recommendations CSV (rec_YYYYMMDD_NNN.csv), fetch actual match results and settle each bet",
        "Calculate per-bet P&L using actual odds (or baseline if real odds weren't available)",
        "Aggregate statistics: total P&L, ROI, win rate, per-market breakdown, Sharpe ratio",
        "Append results to weekend_log.md with structured format",
        "Detect ECE drift: compare predicted probabilities vs actual outcomes per market",
        "CLI command: python -m src.monitoring.bet_result_tracker --settle rec_20260218_001.csv",
        "Unit tests for settlement logic, P&L calculation, edge cases (cancelled matches, void bets)"
      ],
      "last_modified": "",
      "iteration_count": 0,
      "test_status": "",
      "owner_agent": "",
      "notes": "Currently manual process. Live performance tracking (346 bets, +12.3% ROI) is maintained by hand in weekend_log.md."
    },
    {
      "name": "deployment_validator",
      "file_path": "src/ml/deployment_validator.py",
      "status": "draft",
      "depends_on": ["src/ml/model_loader.py", "src/ml/calibration_validator.py"],
      "blocked_by": [],
      "estimated_complexity": "low",
      "known_risks": [
        "Deployment criteria may need adjustment as we learn from live performance",
        "Model files on HF Hub may be out of sync with local — need to verify consistency"
      ],
      "acceptance_criteria": [
        "Pre-deployment checklist validation: ECE < 0.10, holdout bets >= 20, holdout ROI > +30%, FVA > 0",
        "Validate sniper_deployment.json consistency: all referenced model files exist in models/, all listed features exist in features parquet",
        "Cross-reference with live performance data: warn if live ROI contradicts backtest ROI by more than 50pp",
        "Seed robustness check: verify both seed=42 and seed=123 results exist and gap < 30pp",
        "CLI command: python -m src.ml.deployment_validator --check config/sniper_deployment.json",
        "Output structured JSON report with pass/fail per market and specific failure reasons",
        "Unit tests for each validation rule"
      ],
      "last_modified": "",
      "iteration_count": 0,
      "test_status": "",
      "owner_agent": "",
      "notes": "Currently deployment validation is a manual checklist in CLAUDE.md. Today's prediction run had MODEL MISMATCH warnings because models referenced in config don't exist locally."
    }
  ],
  "skills_reference": {
    "_guidance": "Links to skill files, style guides, or reference docs that agents should consult for domain-specific patterns.",
    "files": [
      "CLAUDE.md — Project instructions, pipeline commands, architecture overview, critical reminders",
      "docs/OPTIMIZATION_ANALYSIS_PROMPT.md — Guide for analyzing CI optimization results",
      "docs/book_notes/plans/ — 7 implementation plans from domain reference material",
      "config/strategies.yaml — Betting thresholds, risk management, monotonic constraints",
      "config/sniper_deployment.json — Current production deployment config"
    ]
  },
  "orchestration": {
    "_guidance": "How agents are actually run.",
    "method": "claude_code_agent_team",
    "method_options": [
      "claude_code_agent_team: Claude Code spawns teammates that coordinate via shared task list and direct messaging. Recommended for complex builds.",
      "claude_code_subagents: Single session kicks off isolated background workers. Good for simple parallel tasks like research or file exploration.",
      "manual_sequential: You paste context into each AI conversation manually. Works anywhere, no tooling required.",
      "human_in_loop: Human reviews and approves between each agent step."
    ],
    "claude_code_agent_team": {
      "_guidance": "Configuration for Claude Code's experimental agent teams feature.",
      "setup": {
        "required_file": ".claude/settings.json",
        "required_content": {
          "env": {
            "CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS": "1"
          }
        },
        "project_structure": [
          ".claude/settings.json    — enables agent teams",
          "CLAUDE.md                — project context auto-loaded by all teammates",
          "agentspec.json           — this file (source of truth, referenced by CLAUDE.md)"
        ]
      },
      "claude_md_should_contain": [
        "Project overview and description",
        "Project invariants (copy from this file or reference agentspec.json)",
        "Coding conventions summary",
        "Dependency policy",
        "Key terminology definitions",
        "Instruction: 'Read agentspec.json for full project governance and agent specifications'"
      ],
      "team_structure": {
        "team_lead": "Coordinates work, assigns tasks, synthesizes results. Use delegation mode (Shift+Tab) to prevent the lead from implementing — it should only coordinate.",
        "teammates": [
          {
            "role": "coding_agent",
            "description": "Builds and modifies code following project invariants, coding conventions, and dependency policy. Works on one module at a time. Must run pytest on affected test files before declaring work complete.",
            "suggested_model": "opus"
          },
          {
            "role": "validation_agent",
            "description": "Reviews coding agent output for correctness, convention compliance, security, and invariant violations. Runs black --check, isort --check-only, pytest. Does not fix code — only identifies and classifies problems with severity ratings.",
            "suggested_model": "sonnet"
          },
          {
            "role": "devils_advocate",
            "description": "Stress-tests design decisions, assumptions, and edge cases. Asks: what happens when the API is down? What if odds change between fetch and bet? What assumption about data freshness could be wrong? Every finding must include a concrete recommendation.",
            "suggested_model": "opus"
          }
        ]
      },
      "launch_prompt_template": "Read agentspec.json for full project context and agent specifications. Create an agent team with three teammates: a coding agent (use Opus), a validation agent (use Sonnet), and a devil's advocate (use Opus). Use delegation mode — the lead should coordinate only, not implement. Start with [MODULE_NAME] from the modules list. The coding agent builds, the validation agent reviews for correctness and convention compliance, and the devil's advocate critiques design decisions. No module advances to 'stable' with unresolved critical or warning findings. Max 3 correction iterations before escalating to me.",
      "important_notes": [
        "Teammates do NOT inherit the lead's conversation history — all context comes from CLAUDE.md and the spawn prompt",
        "Clearly separate file ownership per teammate to avoid edit conflicts",
        "Use Shift+Up/Down to cycle between teammates, Enter to view their session",
        "The lead may try to implement instead of delegating — use delegation mode (Shift+Tab) or tell it to wait",
        "One team per session — clean up the current team before starting a new one",
        "No session resumption — if you close the session, teammates are lost. Start fresh.",
        "Task status can lag — if something looks stuck, check the teammate directly"
      ]
    }
  },
  "context_strategy": {
    "max_lines_per_prompt": 300,
    "priority_order": [
      "project_invariants",
      "ai_principles",
      "relevant_module",
      "coding_conventions",
      "architecture_summary",
      "domain_context"
    ],
    "always_include": [
      "project_invariants",
      "ai_principles",
      "project_overview",
      "coding_conventions"
    ],
    "include_when_relevant": [
      "decision_log",
      "dependent_modules",
      "domain_context",
      "error_handling_standard",
      "testing_strategy",
      "skills_reference"
    ],
    "never_include_together": [
      "workflow_diagram",
      "full_module_list"
    ]
  },
  "agent_prompts": {
    "coding_agent": {
      "system_prompt_template": "You are a senior ML engineer and quantitative betting strategist with 15 years of experience. You write clean, modular, production-grade Python code. You follow the project's architecture, coding conventions, dependency policy, and project invariants exactly. You never add features that weren't requested. You never add dependencies without approval. You never rename established terminology. You are paranoid about data leakage and always verify temporal ordering. You check for _x/_y columns after merges. You use TimeSeriesSplit, never StratifiedKFold with shuffle. When uncertain, you state your assumptions explicitly rather than guessing silently.",
      "context_injection_order": [
        "project_overview",
        "project_invariants",
        "coding_conventions",
        "domain_context",
        "relevant_module",
        "dependent_modules",
        "skills_reference",
        "ai_principles"
      ],
      "output_format": "Return code in fenced blocks with file path as the first comment line. Include inline comments only where logic is non-obvious. After code, provide a brief changelog of what was added or modified and why."
    },
    "validation_agent": {
      "system_prompt_template": "You are a meticulous QA engineer specializing in ML pipeline validation. Your job is to break things before they reach production. You run every check systematically and report results in structured format. You do not fix code — you only identify and classify problems. You verify against: python -m py_compile, pytest, black --check, isort --check-only, acceptance criteria, coding conventions, dependency policy, error handling standards, and project invariants. You are especially vigilant about data leakage, _x/_y column collisions, temporal ordering violations, and calibration integrity.",
      "expected_output_format": {
        "module": "",
        "checks_run": [],
        "findings": [
          {
            "severity": "critical | warning | note",
            "category": "compile | test | architecture | edge_case | style | convention | dependency | invariant_violation | security | data_leakage",
            "description": "",
            "suggested_fix": "",
            "line_reference": ""
          }
        ],
        "verdict": "pass | fail | pass_with_warnings"
      }
    },
    "devils_advocate_agent": {
      "system_prompt_template": "You are a senior architect and technical skeptic who has seen betting models fail from subtle data leakage, overfitting to specific seeds, calibration drift, and untested edge cases. Your role is not to write code but to stress-test decisions. You ask: 'What happens when the odds API is down?' 'What if features parquet is 2 weeks stale?' 'Are we silently producing fake edges with baseline odds?' 'Will this degrade gracefully or crash the entire prediction pipeline?' You provide structured, actionable critiques — not vague concerns. Every finding must include a concrete recommendation.",
      "expected_output_format": {
        "module": "",
        "findings": [
          {
            "severity": "critical | warning | note",
            "category": "design | scalability | security | assumption | edge_case | maintainability | data_leakage | calibration",
            "concern": "",
            "worst_case_scenario": "",
            "recommendation": ""
          }
        ],
        "overall_confidence": "high | medium | low",
        "summary": ""
      }
    }
  },
  "ai_team_guidelines": {
    "_guidance": "Behavioral rules for AI agents.",
    "when_to_use_multiple_agents": [
      "Large modules or code chunks exceeding 300 lines",
      "Tasks with multiple independent responsibilities (e.g., build + test + review)",
      "Complex refactors requiring separate verification loops",
      "New feature engineers that need leakage verification"
    ],
    "when_to_use_agent_teams_vs_subagents": {
      "use_agent_teams": "When teammates need to coordinate, share context, or challenge each other's work. Best for new modules, complex refactors, and anything touching the training pipeline.",
      "use_subagents": "When tasks are isolated: CI result analysis, data exploration, single file fixes, documentation updates."
    },
    "rules_for_coordination": [
      "Each agent receives only the relevant module/context to minimize token usage",
      "Validation agent always verifies before module can advance",
      "Devil's advocate runs after validation, before module is finalized",
      "Only one agent modifies a module at a time to prevent conflicts",
      "All agents follow coding_conventions, dependency_policy, error_handling_standard, and project_invariants",
      "Coding agent must run pytest on affected files before declaring work complete",
      "Validation agent must run black --check and isort --check-only",
      "No module advances to stable with unresolved critical or warning findings"
    ],
    "max_correction_iterations": 3,
    "fallback_on_max": "Escalate to human with summary of unresolved issues, decision log entry, and recommended next steps"
  },
  "build_flow": {
    "_guidance": "Project-specific execution sequence.",
    "steps": [
      "1. Coding agent reads module spec from agentspec.json modules list",
      "2. Coding agent reads dependent modules and skills_reference files for context",
      "3. Coding agent implements module following acceptance criteria",
      "4. Coding agent runs pytest on new/affected test files",
      "5. Validation agent reviews: compile check, tests, conventions, invariants, leakage checks",
      "6. Devil's advocate critiques: design decisions, failure modes, edge cases, assumptions",
      "7. If findings exist: coding agent addresses them (max 3 iterations)",
      "8. If all clear: update module status to stable, log decision",
      "9. If max iterations reached: escalate to human with findings summary"
    ]
  },
  "validation_flow": {
    "compile_checks": [
      "python -m py_compile {file_path}"
    ],
    "tests": [
      "pytest tests/ -x --tb=short",
      "pytest tests/test_data_leakage.py -v"
    ],
    "quality_checks": [
      "black --check src/ experiments/ scripts/",
      "isort --check-only src/ experiments/ scripts/"
    ],
    "security_checks": [
      "No hardcoded API keys, tokens, or credentials in source files",
      "No .env files committed to git",
      "Data leakage audit: verify no future information in features (tests/test_data_leakage.py)",
      "Validate _x/_y columns don't exist after merge operations"
    ],
    "severity_levels": {
      "critical": "Blocks module from advancing. Must fix before proceeding. Examples: data leakage, test failures, _x/_y columns, calibration violations.",
      "warning": "Should fix before marking stable. Can proceed to next iteration. Examples: missing type hints, overly broad exception handling, suboptimal caching.",
      "note": "Optional improvement. Log for future consideration. Examples: code style preferences, documentation improvements, performance optimizations."
    },
    "gate_policy": "Module cannot reach 'stable' with any critical or warning findings unresolved"
  },
  "ai_principles": {
    "principles": [
      {
        "principle": "Keep code modular and small (one file / one responsibility)",
        "reasoning": "Smaller, modular code makes it easier for AI to reason about the project"
      },
      {
        "principle": "Avoid duplication; reuse functions and components",
        "reasoning": "Reduces maintenance burden and prevents drift between duplicated logic"
      },
      {
        "principle": "Focus on core functionality before optional features",
        "reasoning": "Keeps early iterations lean and testable before adding complexity"
      },
      {
        "principle": "Use built-in frameworks and declarative approaches where possible",
        "reasoning": "High-level abstractions allow scalability and maintainability"
      },
      {
        "principle": "Maintain clean architecture patterns to reduce boilerplate",
        "reasoning": "Lean code and clear structure reduce cognitive load and token usage"
      },
      {
        "principle": "Limit initial context to a manageable size (~300 lines) for AI efficiency",
        "reasoning": "Limiting context reduces noise and improves precision of AI output"
      },
      {
        "principle": "Use feedback loops to allow AI self-correction",
        "reasoning": "Validation loops help the AI iteratively fix errors without human intervention"
      },
      {
        "principle": "Respect established terminology and architecture invariants over new abstractions",
        "reasoning": "Prevents session drift and maintains project consistency across long builds"
      },
      {
        "principle": "Be paranoid about data leakage — verify temporal ordering at every step",
        "reasoning": "Data leakage has been the root cause of multiple false-positive results in this project"
      }
    ]
  },
  "decision_log": [
    {
      "date": "2026-02-18",
      "decision": "Populated agentspec.json with 6 modules targeting production robustness gaps identified from live prediction pipeline failures",
      "alternatives_considered": [
        "Focus modules on new feature engineers (rejected: features are mature at 608 columns)",
        "Focus modules on H2H market recovery (rejected: H2H consistently fails, low expected value)",
        "Focus modules on model architecture changes (rejected: agreement ensembles already winning)"
      ],
      "reasoning": "Live prediction pipeline has 17 warnings per run, uses fake odds for all niche markets, missed Arsenal vs Wolves due to stale data, and has no automated result tracking. Production robustness > model improvements at this stage.",
      "module_affected": "all"
    }
  ],
  "project_health": {
    "total_modules": 6,
    "stable": 0,
    "in_progress": 0,
    "draft": 6,
    "blocked": 0,
    "current_iteration_focus": "live_odds_client — highest impact: enables real edge calculation for all niche market predictions",
    "last_escalation": ""
  },
  "handoff_protocol": {
    "resumption_prompt": "Read project_overview, project_invariants, project_health, and all modules with status 'in-progress'. Review the last 3 entries in decision_log. Summarize current project state and confirm understanding before proceeding.",
    "context_minimum_for_resumption": [
      "project_overview",
      "project_invariants",
      "modules",
      "project_health",
      "decision_log"
    ],
    "stale_threshold_days": 7,
    "stale_action": "Re-validate all in-progress modules before continuing. Flag any modules whose dependencies have changed since last session."
  },
  "workflow_diagram": {
    "_guidance": "Human-readable visualization only. Not operational logic.",
    "format": "mermaid",
    "diagram": "graph TD\n  A[Load JSON Context + Invariants] --> B[Select Module / Feature to Modify]\n  B --> C[Coding Agent Writes / Modifies Code]\n  C --> D[Validation Agent Reviews]\n  D --> E[Devil's Advocate Agent Critiques]\n  E --> F{All Findings Resolved?}\n  F -->|Yes - Pass| G[Update Module Status to Stable]\n  F -->|No - Issues Found| H[Feedback to Coding Agent]\n  G --> I[Log Decision & Update Project Health]\n  I --> J[Feed Next Module]\n  H --> K{Max Iterations Reached?}\n  K -->|No| C\n  K -->|Yes| L[Escalate to Human + Log Decision]\n  J --> B",
    "description": "General agent loop: load context → code → validate → critique → gate → advance or iterate. Invariants are loaded at every step."
  },
  "_instruction": "When you read this file, create an agent team using the orchestration.claude_code_agent_team section. Use delegation mode. Start with the first module in the modules list unless told otherwise."
}
