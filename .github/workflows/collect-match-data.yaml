name: Data Pipeline - Collect & Process

on:
  schedule:
    - cron: '0 3 * * 1'

  workflow_dispatch:
    inputs:
      collection_mode:
        description: 'Collection mode'
        required: false
        default: 'update'
        type: choice
        options:
          - update
          - bulk
          - season
      league:
        description: 'League to process'
        required: false
        default: 'premier_league'
        type: choice
        options:
          - premier_league
          - la_liga
          - bundesliga
          - serie_a
          - ligue_1
      season:
        description: 'Season (for update/season mode)'
        required: false
        default: '2025'
        type: string
      start_season:
        description: 'Start season (for bulk mode)'
        required: false
        default: '2023'
        type: string
      end_season:
        description: 'End season (for bulk mode)'
        required: false
        default: '2025'
        type: string
      days_back:
        description: 'Days back (for update mode)'
        required: false
        default: '26'
        type: string
      include_details:
        description: 'Include lineups, events, player stats'
        required: false
        default: true
        type: boolean
      skip_features:
        description: 'Skip feature generation'
        required: false
        default: true
        type: boolean

permissions:
  contents: read

jobs:
  collect-data:
    runs-on: ubuntu-latest

    steps:
      - name: Verify secrets
        run: |
          if [ -z "${{ secrets.HF_TOKEN }}" ]; then
            echo "::error::HF_TOKEN secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.API_FOOTBALL_KEY }}" ]; then
            echo "::error::API_FOOTBALL_KEY secret is not set"
            exit 1
          fi
          echo "âœ… All required secrets are configured"

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Cache UV dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Set up Python
        run: uv python install 3.10

      - name: Install dependencies
        run: uv sync --frozen

      - name: Create .env file with API key
        run: |
          echo "API_FOOTBALL_KEY=${{ secrets.API_FOOTBALL_KEY }}" > .env
          echo "API_BASE_URL=${{ vars.API_BASE_URL }}" >> .env
          echo "DAILY_LIMIT=${{ vars.DAILY_LIMIT }}" >> .env
          echo "PER_MIN_LIMIT=${{ vars.PER_MIN_LIMIT }}" >> .env
          echo "STATE_PATH=${{ vars.STATE_PATH }}" >> .env

      - name: Create data directories
        run: |
          mkdir -p data/01-raw
          mkdir -p data/02-preprocessed
          mkdir -p data/03-features

      - name: Verify API connectivity
        run: |
          uv run python -c "
          from src.data_collection.api_client import FootballAPIClient
          client = FootballAPIClient()
          remaining = client.daily_limit - client.state.get('count', 0)
          print(f'âœ… API connection OK')
          print(f'ðŸ“Š Daily limit: {client.daily_limit}')
          print(f'ðŸ“ˆ Used today: {client.state.get(\"count\", 0)}')
          print(f'ðŸ’š Remaining: {remaining}')
          if remaining < 50:
              print('::warning::Low API quota remaining!')
          "

      - name: Download existing data from HF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python entrypoints/download_data.py

      - name: Save raw data checksum
        id: checksum_before
        run: |
          if [ -d "data/01-raw" ]; then
            CHECKSUM=$(find data/01-raw -type f -name "*.parquet" -exec md5sum {} \; | sort | md5sum | cut -d' ' -f1)
            echo "checksum=$CHECKSUM" >> $GITHUB_OUTPUT
            echo "ðŸ“ Raw data checksum before: $CHECKSUM"
          else
            echo "checksum=none" >> $GITHUB_OUTPUT
          fi

      - name: Run match collector
        id: collector
        run: |
          MODE="${{ github.event.inputs.collection_mode || 'update' }}"
          LEAGUE="${{ github.event.inputs.league || 'premier_league' }}"
          SEASON="${{ github.event.inputs.season || '2025' }}"
          START_SEASON="${{ github.event.inputs.start_season || '2023' }}"
          END_SEASON="${{ github.event.inputs.end_season || '2025' }}"
          DAYS_BACK="${{ github.event.inputs.days_back || '26' }}"
          INCLUDE_DETAILS="${{ github.event.inputs.include_details || 'true' }}"

          DETAILS_FLAG=""
          if [ "$INCLUDE_DETAILS" = "true" ]; then
            DETAILS_FLAG="--include-all"
          fi

          echo "ðŸ“¥ Collection mode: $MODE"
          echo "ðŸ“¥ League: $LEAGUE"

          if [ "$MODE" = "bulk" ]; then
            echo "ðŸ“¥ Collecting seasons $START_SEASON to $END_SEASON..."
            uv run python entrypoints/collect.py \
              --mode bulk \
              --league $LEAGUE \
              --start-season $START_SEASON \
              --end-season $END_SEASON \
              $DETAILS_FLAG
          elif [ "$MODE" = "season" ]; then
            echo "ðŸ“¥ Collecting full season $SEASON..."
            uv run python entrypoints/collect.py \
              --mode season \
              --league $LEAGUE \
              --season $SEASON \
              $DETAILS_FLAG
          else
            echo "ðŸ“¥ Smart update for season $SEASON, last $DAYS_BACK days..."
            uv run python entrypoints/collect.py \
              --mode update \
              --strategy smart \
              --league $LEAGUE \
              --season $SEASON \
              --days-back $DAYS_BACK
          fi

      - name: Check for data changes
        id: checksum_after
        run: |
          CHECKSUM=$(find data/01-raw -type f -name "*.parquet" -exec md5sum {} \; | sort | md5sum | cut -d' ' -f1)
          echo "checksum=$CHECKSUM" >> $GITHUB_OUTPUT
          echo "ðŸ“ Raw data checksum after: $CHECKSUM"

          if [ "${{ steps.checksum_before.outputs.checksum }}" = "$CHECKSUM" ]; then
            echo "data_changed=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ No data changes detected"
          else
            echo "data_changed=true" >> $GITHUB_OUTPUT
            echo "âœ… Data changes detected"
          fi

      - name: Preprocess data
        if: success() && (steps.checksum_after.outputs.data_changed == 'true' || github.event.inputs.skip_features != 'true')
        run: |
          echo "ðŸ”„ Regenerating FULL dataset from local raw files..."
          uv run python entrypoints/preprocess.py --config config/local.yaml

      - name: Generate features
        if: success() && (steps.checksum_after.outputs.data_changed == 'true' || github.event.inputs.skip_features != 'true')
        run: |
          echo "ðŸŽ¯ Calculating features..."
          uv run python entrypoints/features.py --config config/local.yaml

      - name: Upload data to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 30
          max_attempts: 3
          retry_wait_seconds: 30
          command: |
            MODE="${{ github.event.inputs.collection_mode || 'update' }}"
            LEAGUE="${{ github.event.inputs.league || 'premier_league' }}"

            echo "â˜ï¸ Uploading data to HF Hub..."
            if [ "$MODE" = "bulk" ]; then
              # Upload all seasons for the league
              for season_dir in data/01-raw/$LEAGUE/*/; do
                SEASON=$(basename "$season_dir")
                echo "  Uploading season $SEASON..."
                uv run python entrypoints/upload_data.py --season $SEASON || true
              done
            else
              SEASON="${{ github.event.inputs.season || '2025' }}"
              uv run python entrypoints/upload_data.py --season $SEASON
            fi

      - name: Create summary
        if: always()
        run: |
          MODE="${{ github.event.inputs.collection_mode || 'update' }}"
          LEAGUE="${{ github.event.inputs.league || 'premier_league' }}"
          SEASON="${{ github.event.inputs.season || '2025' }}"
          START_SEASON="${{ github.event.inputs.start_season || '2023' }}"
          END_SEASON="${{ github.event.inputs.end_season || '2025' }}"
          DATA_CHANGED="${{ steps.checksum_after.outputs.data_changed }}"

          echo "## ðŸ“Š Data Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ job.status }}" = "success" ]; then
            echo "âœ… **Pipeline completed successfully**" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Pipeline failed**" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### âš™ï¸ Parameters" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Mode | \`$MODE\` |" >> $GITHUB_STEP_SUMMARY
          echo "| League | \`$LEAGUE\` |" >> $GITHUB_STEP_SUMMARY
          if [ "$MODE" = "bulk" ]; then
            echo "| Seasons | \`$START_SEASON - $END_SEASON\` |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Season | \`$SEASON\` |" >> $GITHUB_STEP_SUMMARY
          fi
          echo "| Data changed | \`$DATA_CHANGED\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### ðŸ“ˆ Data Metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Raw data metrics - show all seasons for the league
          if [ -d "data/01-raw/$LEAGUE" ]; then
            echo "**Raw Data** (\`data/01-raw/$LEAGUE/\`):" >> $GITHUB_STEP_SUMMARY
            TOTAL_FIXTURES=0
            for season_dir in data/01-raw/$LEAGUE/*/; do
              if [ -d "$season_dir" ]; then
                S=$(basename "$season_dir")
                SIZE=$(du -sh "$season_dir" 2>/dev/null | cut -f1 || echo "N/A")
                COUNT=$(python3 -c "import pandas as pd; print(len(pd.read_parquet('$season_dir/matches.parquet')))" 2>/dev/null || echo "0")
                echo "- Season \`$S\`: \`$COUNT\` fixtures (\`$SIZE\`)" >> $GITHUB_STEP_SUMMARY
                TOTAL_FIXTURES=$((TOTAL_FIXTURES + COUNT))
              fi
            done
            echo "- **Total**: \`$TOTAL_FIXTURES\` fixtures" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Features metrics
          if [ -f "data/03-features/features.csv" ]; then
            FEAT_SIZE=$(du -sh "data/03-features/features.csv" 2>/dev/null | cut -f1 || echo "N/A")
            FEAT_ROWS=$(wc -l < "data/03-features/features.csv" 2>/dev/null || echo "N/A")
            FEAT_COLS=$(head -1 "data/03-features/features.csv" 2>/dev/null | tr ',' '\n' | wc -l || echo "N/A")
            echo "**Features** (\`data/03-features/\`):" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“¦ Size: \`$FEAT_SIZE\`" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“Š Shape: \`$FEAT_ROWS rows Ã— $FEAT_COLS columns\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # API usage
          if [ -f "state.json" ]; then
            API_COUNT=$(python3 -c "import json; print(json.load(open('state.json')).get('count', 'N/A'))" 2>/dev/null || echo "N/A")
            echo "### ðŸ”Œ API Usage" >> $GITHUB_STEP_SUMMARY
            echo "- Requests today: \`$API_COUNT\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "### ðŸ“‹ Pipeline Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. âœ… **Verify** - Secrets & API connectivity" >> $GITHUB_STEP_SUMMARY
          echo "2. ðŸ“¥ **Download** - Sync from Hugging Face" >> $GITHUB_STEP_SUMMARY
          echo "3. ðŸ”„ **Collect** - Smart update from API" >> $GITHUB_STEP_SUMMARY
          echo "4. ðŸ§¹ **Preprocess** - Raw â†’ Processed Parquet" >> $GITHUB_STEP_SUMMARY
          echo "5. ðŸŽ¯ **Features** - Generate ML features" >> $GITHUB_STEP_SUMMARY
          echo "6. â˜ï¸ **Upload** - Push to Hugging Face" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f fixtures_updater.log ]; then
            echo "### ðŸ“œ Recent Logs" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            tail -15 fixtures_updater.log >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: collector-logs
          path: |
            fixtures_updater.log
            state.json
          retention-days: 30
