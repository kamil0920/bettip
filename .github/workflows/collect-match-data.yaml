name: Data Pipeline - Collect & Process

on:
  schedule:
    - cron: '0 3 * * 1'

  workflow_dispatch:
    inputs:
      days_back:
        description: 'Number of days back to download'
        required: false
        default: '10'
        type: string
      season:
        description: 'Season (year of start)'
        required: false
        default: '2025'
        type: string
      league:
        description: 'League to process'
        required: false
        default: 'premier_league'
        type: choice
        options:
          - premier_league
          - la_liga
          - bundesliga
          - serie_a
          - ligue_1
      force_all_seasons:
        description: 'Force reprocessing ALL seasons defined in config?'
        required: false
        default: false
        type: boolean
      skip_features:
        description: 'Skip feature generation if no data changes'
        required: false
        default: true
        type: boolean

permissions:
  contents: read

jobs:
  collect-data:
    runs-on: ubuntu-latest

    steps:
      - name: Verify secrets
        run: |
          if [ -z "${{ secrets.HF_TOKEN }}" ]; then
            echo "::error::HF_TOKEN secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.API_FOOTBALL_KEY }}" ]; then
            echo "::error::API_FOOTBALL_KEY secret is not set"
            exit 1
          fi
          echo "âœ… All required secrets are configured"

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Cache UV dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Set up Python
        run: uv python install 3.10

      - name: Install dependencies
        run: uv sync --frozen

      - name: Create .env file with API key
        run: |
          echo "API_FOOTBALL_KEY=${{ secrets.API_FOOTBALL_KEY }}" > .env
          echo "API_BASE_URL=${{ vars.API_BASE_URL }}" >> .env
          echo "DAILY_LIMIT=${{ vars.DAILY_LIMIT }}" >> .env
          echo "PER_MIN_LIMIT=${{ vars.PER_MIN_LIMIT }}" >> .env
          echo "STATE_PATH=${{ vars.STATE_PATH }}" >> .env

      - name: Create data directories
        run: |
          mkdir -p data/01-raw
          mkdir -p data/02-preprocessed
          mkdir -p data/03-features

      - name: Verify API connectivity
        run: |
          uv run python -c "
          from src.data_collection.api_client import FootballAPIClient
          client = FootballAPIClient()
          remaining = client.daily_limit - client.state.get('count', 0)
          print(f'âœ… API connection OK')
          print(f'ðŸ“Š Daily limit: {client.daily_limit}')
          print(f'ðŸ“ˆ Used today: {client.state.get(\"count\", 0)}')
          print(f'ðŸ’š Remaining: {remaining}')
          if remaining < 50:
              print('::warning::Low API quota remaining!')
          "

      - name: Download existing data from HF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python entrypoints/download_data.py

      - name: Save raw data checksum
        id: checksum_before
        run: |
          if [ -d "data/01-raw" ]; then
            CHECKSUM=$(find data/01-raw -type f -name "*.parquet" -exec md5sum {} \; | sort | md5sum | cut -d' ' -f1)
            echo "checksum=$CHECKSUM" >> $GITHUB_OUTPUT
            echo "ðŸ“ Raw data checksum before: $CHECKSUM"
          else
            echo "checksum=none" >> $GITHUB_OUTPUT
          fi

      - name: Run match collector
        id: collector
        run: |
          DAYS_BACK="${{ github.event.inputs.days_back || '26' }}"
          SEASON="${{ github.event.inputs.season || '2025' }}"
          LEAGUE="${{ github.event.inputs.league || 'premier_league' }}"

          echo "ðŸ“¥ Collecting data for $LEAGUE season $SEASON, last $DAYS_BACK days..."
          uv run python entrypoints/collect.py \
            --mode update \
            --strategy smart \
            --league $LEAGUE \
            --season $SEASON \
            --days-back $DAYS_BACK

      - name: Check for data changes
        id: checksum_after
        run: |
          CHECKSUM=$(find data/01-raw -type f -name "*.parquet" -exec md5sum {} \; | sort | md5sum | cut -d' ' -f1)
          echo "checksum=$CHECKSUM" >> $GITHUB_OUTPUT
          echo "ðŸ“ Raw data checksum after: $CHECKSUM"

          if [ "${{ steps.checksum_before.outputs.checksum }}" = "$CHECKSUM" ]; then
            echo "data_changed=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ No data changes detected"
          else
            echo "data_changed=true" >> $GITHUB_OUTPUT
            echo "âœ… Data changes detected"
          fi

      - name: Preprocess data
        if: success() && (steps.checksum_after.outputs.data_changed == 'true' || github.event.inputs.skip_features != 'true')
        run: |
          echo "ðŸ”„ Regenerating FULL dataset from local raw files..."
          uv run python entrypoints/preprocess.py --config config/local.yaml

      - name: Generate features
        if: success() && (steps.checksum_after.outputs.data_changed == 'true' || github.event.inputs.skip_features != 'true')
        run: |
          echo "ðŸŽ¯ Calculating features..."
          uv run python entrypoints/features.py --config config/local.yaml

      - name: Upload data to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_wait_seconds: 30
          command: |
            SEASON="${{ github.event.inputs.season || '2025' }}"
            echo "â˜ï¸ Uploading data to HF Hub..."
            uv run python entrypoints/upload_data.py --season $SEASON

      - name: Create summary
        if: always()
        run: |
          SEASON="${{ github.event.inputs.season || '2025' }}"
          LEAGUE="${{ github.event.inputs.league || 'premier_league' }}"
          DATA_CHANGED="${{ steps.checksum_after.outputs.data_changed }}"

          echo "## ðŸ“Š Data Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ job.status }}" = "success" ]; then
            echo "âœ… **Pipeline completed successfully**" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Pipeline failed**" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### âš™ï¸ Parameters" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| League | \`$LEAGUE\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Season | \`$SEASON\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Days back | \`${{ github.event.inputs.days_back || '26' }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Data changed | \`$DATA_CHANGED\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### ðŸ“ˆ Data Metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Raw data metrics
          if [ -d "data/01-raw/$LEAGUE/$SEASON" ]; then
            RAW_SIZE=$(du -sh "data/01-raw/$LEAGUE/$SEASON" 2>/dev/null | cut -f1 || echo "N/A")
            FIXTURES_COUNT=$(python3 -c "import pandas as pd; print(len(pd.read_parquet('data/01-raw/$LEAGUE/$SEASON/matches.parquet')))" 2>/dev/null || echo "N/A")
            echo "**Raw Data** (\`data/01-raw/\`):" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“¦ Size: \`$RAW_SIZE\`" >> $GITHUB_STEP_SUMMARY
            echo "- âš½ Fixtures: \`$FIXTURES_COUNT\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Preprocessed data metrics
          if [ -d "data/02-preprocessed/$LEAGUE/$SEASON" ]; then
            PREP_SIZE=$(du -sh "data/02-preprocessed/$LEAGUE/$SEASON" 2>/dev/null | cut -f1 || echo "N/A")
            MATCHES_ROWS=$(python3 -c "import pandas as pd; print(len(pd.read_parquet('data/02-preprocessed/$LEAGUE/$SEASON/matches.parquet')))" 2>/dev/null || echo "N/A")
            echo "**Preprocessed Data** (\`data/02-preprocessed/\`):" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“¦ Size: \`$PREP_SIZE\`" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“Š Matches: \`$MATCHES_ROWS\` rows" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Features metrics
          if [ -f "data/03-features/features.csv" ]; then
            FEAT_SIZE=$(du -sh "data/03-features/features.csv" 2>/dev/null | cut -f1 || echo "N/A")
            FEAT_ROWS=$(wc -l < "data/03-features/features.csv" 2>/dev/null || echo "N/A")
            FEAT_COLS=$(head -1 "data/03-features/features.csv" 2>/dev/null | tr ',' '\n' | wc -l || echo "N/A")
            echo "**Features** (\`data/03-features/\`):" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“¦ Size: \`$FEAT_SIZE\`" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“Š Shape: \`$FEAT_ROWS rows Ã— $FEAT_COLS columns\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # API usage
          if [ -f "state.json" ]; then
            API_COUNT=$(python3 -c "import json; print(json.load(open('state.json')).get('count', 'N/A'))" 2>/dev/null || echo "N/A")
            echo "### ðŸ”Œ API Usage" >> $GITHUB_STEP_SUMMARY
            echo "- Requests today: \`$API_COUNT\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "### ðŸ“‹ Pipeline Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. âœ… **Verify** - Secrets & API connectivity" >> $GITHUB_STEP_SUMMARY
          echo "2. ðŸ“¥ **Download** - Sync from Hugging Face" >> $GITHUB_STEP_SUMMARY
          echo "3. ðŸ”„ **Collect** - Smart update from API" >> $GITHUB_STEP_SUMMARY
          echo "4. ðŸ§¹ **Preprocess** - Raw â†’ Processed Parquet" >> $GITHUB_STEP_SUMMARY
          echo "5. ðŸŽ¯ **Features** - Generate ML features" >> $GITHUB_STEP_SUMMARY
          echo "6. â˜ï¸ **Upload** - Push to Hugging Face" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f fixtures_updater.log ]; then
            echo "### ðŸ“œ Recent Logs" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            tail -15 fixtures_updater.log >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: collector-logs
          path: |
            fixtures_updater.log
            state.json
          retention-days: 30
