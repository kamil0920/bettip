name: Data Pipeline - Collect & Process

on:
  schedule:
    - cron: '0 3 * * 1'

  workflow_dispatch:
    inputs:
      days_back:
        description: 'number of days back to download'
        required: false
        default: '10'
        type: string
      season:
        description: 'season (year of start)'
        required: false
        default: '2025'
        type: string
      force_all_seasons:
        description: 'Force reprocessing ALL seasons defined in config?'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: read

jobs:
  collect-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Set up Python
        run: uv python install 3.10

      - name: Install dependencies
        run: uv sync --frozen

      - name: Create .env file with API key
        run: |
          echo "API_FOOTBALL_KEY=${{ secrets.API_FOOTBALL_KEY }}" > .env
          echo "API_BASE_URL=${{ vars.API_BASE_URL }}" >> .env
          echo "DAILY_LIMIT=${{ vars.DAILY_LIMIT }}" >> .env
          echo "PER_MIN_LIMIT=${{ vars.PER_MIN_LIMIT }}" >> .env
          echo "STATE_PATH=${{ vars.STATE_PATH }}" >> .env

      - name: Create data directories
        run: |
          mkdir -p data/01-raw
          mkdir -p data/02-preprocessed
          mkdir -p data/03-features

      - name: Download existing data from HF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python entrypoints/download_data.py

      - name: Run match collector
        run: |
          DAYS_BACK="${{ github.event.inputs.days_back || '26' }}"
          SEASON="${{ github.event.inputs.season || '2025' }}"

          echo "Download data for season $SEASON, last $DAYS_BACK days..."
          uv run python entrypoints/collect.py \
            --mode update \
            --strategy smart \
            --season $SEASON \
            --days-back $DAYS_BACK

      - name: Preprocess data
        if: success()
        run: |
          SEASON="${{ github.event.inputs.season || '2025' }}"
          FORCE_ALL="${{ github.event.inputs.force_all_seasons }}"
          
          if [ "$FORCE_ALL" == "true" ]; then
            echo "ðŸ”„ FULL REPROCESSING: Running for ALL seasons from config..."
            uv run python entrypoints/preprocess.py --config config/local.yaml
          else
            echo "âš¡ INCREMENTAL UPDATE: Running for season $SEASON only..."
            uv run python entrypoints/preprocess.py --config config/local.yaml --seasons $SEASON
          fi

      - name: Generate features
        if: success()
        run: |
          SEASON="${{ github.event.inputs.season || '2025' }}"
          FORCE_ALL="${{ github.event.inputs.force_all_seasons }}"
          
          if [ "$FORCE_ALL" == "true" ]; then
            echo "ðŸ”„ FULL FEATURE GEN: Running for ALL seasons from config..."
            uv run python entrypoints/features.py --config config/local.yaml
          else
            echo "âš¡ INCREMENTAL FEATURE GEN: Running for season $SEASON only..."
            uv run python entrypoints/features.py --config config/local.yaml --seasons $SEASON
          fi

      - name: Upload data to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          SEASON="${{ github.event.inputs.season || '2025' }}"
          echo "Uploading data to HF Hub..."
          
          uv run python entrypoints/upload_data.py --season $SEASON

      - name: Create summary
        if: always()
        run: |
          echo "## Data Collection & Processing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "âœ… Pipeline completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ’¾ Data synced with Hugging Face Hub" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Pipeline Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. ðŸ“¥ **Collect** - Smart update of match data" >> $GITHUB_STEP_SUMMARY
          echo "2. ðŸ”„ **Preprocess** - Transform JSON to Parquet" >> $GITHUB_STEP_SUMMARY
          echo "3. ðŸŽ¯ **Features** - Generate ML-ready features" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Parameters" >> $GITHUB_STEP_SUMMARY
          echo "- **Season**: ${{ github.event.inputs.season || '2025' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Days back**: ${{ github.event.inputs.days_back || '26' }}" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Data Directories" >> $GITHUB_STEP_SUMMARY
          echo "- \`data/01-raw/\` - Raw JSON from API" >> $GITHUB_STEP_SUMMARY
          echo "- \`data/02-preprocessed/\` - Cleaned Parquet files" >> $GITHUB_STEP_SUMMARY
          echo "- \`data/03-features/\` - ML features (CSV)" >> $GITHUB_STEP_SUMMARY

          if [ -f fixtures_updater.log ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Last log lines" >> $GITHUB_STEP_SUMMARY
            echo '```'
            tail -20 fixtures_updater.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: collector-logs
          path: |
            fixtures_updater.log
            state.json
          retention-days: 30
