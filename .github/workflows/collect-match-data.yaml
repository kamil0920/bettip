name: Data Pipeline - Collect & Process

on:
  schedule:
    # 500 req/day budget. Split 13 leagues across Mon-Thu (3-4 leagues/day).
    # Fri: niche odds only (no match collection).
    - cron: '0 3 * * 1'  # Monday: premier_league, la_liga, eredivisie
    - cron: '0 3 * * 2'  # Tuesday: serie_a, bundesliga, portuguese_liga
    - cron: '0 3 * * 3'  # Wednesday: ligue_1, ekstraklasa, belgian_pro_league, scottish_premiership
    - cron: '0 3 * * 4'  # Thursday: turkish_super_lig, liga_mx, mls
    - cron: '0 3 * * 5'  # Friday: niche odds for weekend fixtures

  workflow_dispatch:
    inputs:
      collection_mode:
        description: 'Collection mode'
        required: false
        default: 'update'
        type: choice
        options:
          - update
          - bulk
          - season
      league:
        description: 'League to process (all = all configured leagues)'
        required: false
        default: 'auto'
        type: choice
        options:
          - auto
          - all
          - premier_league
          - la_liga
          - serie_a
          - bundesliga
          - ligue_1
          - ekstraklasa
          - eredivisie
          - portuguese_liga
          - belgian_pro_league
          - scottish_premiership
          - turkish_super_lig
          - liga_mx
          - mls
      season:
        description: 'Season (for update/season mode)'
        required: false
        default: '2025'
        type: string
      start_season:
        description: 'Start season (for bulk mode)'
        required: false
        default: '2023'
        type: string
      end_season:
        description: 'End season (for bulk mode)'
        required: false
        default: '2025'
        type: string
      days_back:
        description: 'Days back (for update mode)'
        required: false
        default: '7'
        type: string
      include_details:
        description: 'Include lineups, events, player stats'
        required: false
        default: true
        type: boolean
      skip_features:
        description: 'Skip feature generation'
        required: false
        default: false
        type: boolean
      backfill_cards:
        description: 'Re-fetch fixtures missing yellow/red card columns in match_stats'
        required: false
        default: false
        type: boolean

permissions:
  contents: read

env:
  CONFIGURED_LEAGUES: "bundesliga ekstraklasa la_liga liga_mx ligue_1 mls premier_league serie_a eredivisie portuguese_liga belgian_pro_league scottish_premiership turkish_super_lig"
  HF_REPO_ID: ${{ vars.HF_REPO_ID }}

jobs:
  collect-data:
    runs-on: ubuntu-latest

    steps:
      - name: Verify secrets
        run: |
          if [ -z "${{ secrets.HF_TOKEN }}" ]; then
            echo "::error::HF_TOKEN secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.API_FOOTBALL_KEY }}" ]; then
            echo "::error::API_FOOTBALL_KEY secret is not set"
            exit 1
          fi
          echo "All required secrets are configured"

      - uses: actions/checkout@v4
      - name: Setup Python environment
        uses: ./.github/actions/setup-python-uv
        with:
          fetch-depth: "0"

      - name: Create .env file with API key
        run: |
          echo "API_FOOTBALL_KEY=${{ secrets.API_FOOTBALL_KEY }}" > .env
          echo "API_BASE_URL=${{ vars.API_BASE_URL }}" >> .env
          echo "DAILY_LIMIT=${{ vars.DAILY_LIMIT || '500' }}" >> .env
          echo "PER_MIN_LIMIT=${{ vars.PER_MIN_LIMIT || '30' }}" >> .env
          echo "STATE_PATH=${{ vars.STATE_PATH }}" >> .env
          echo ".env file created"

      - name: Create data directories
        run: |
          mkdir -p data/01-raw
          mkdir -p data/02-preprocessed
          mkdir -p data/03-features
          mkdir -p data/odds-cache
          mkdir -p data/niche_odds_cache

      - name: Verify API connectivity
        run: |
          uv run python -c "
          from src.data_collection.api_client import FootballAPIClient
          client = FootballAPIClient()
          remaining = client.daily_limit - client.state.get('count', 0)
          print(f'API connection OK')
          print(f'Daily limit: {client.daily_limit}')
          print(f'Used today: {client.state.get(\"count\", 0)}')
          print(f'Remaining: {remaining}')
          if remaining < 50:
              print('::warning::Low API quota remaining!')
          "

      - name: Download existing data from HF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python entrypoints/download_data.py

      - name: Save raw data checksum
        id: checksum_before
        run: |
          if [ -d "data/01-raw" ]; then
            CHECKSUM=$(find data/01-raw -type f -name "*.parquet" -exec md5sum {} \; | sort | md5sum | cut -d' ' -f1)
            echo "checksum=$CHECKSUM" >> $GITHUB_OUTPUT
            echo "Raw data checksum before: $CHECKSUM"
          else
            echo "checksum=none" >> $GITHUB_OUTPUT
          fi

      - name: Determine leagues to process
        id: leagues
        run: |
          LEAGUE_INPUT="${{ github.event.inputs.league || 'auto' }}"

          if [ "$LEAGUE_INPUT" = "all" ]; then
            LEAGUES="${CONFIGURED_LEAGUES}"
            echo "Processing all configured leagues: $LEAGUES"
          elif [ "$LEAGUE_INPUT" = "auto" ]; then
            # Auto-select leagues based on day of week (free tier: 2 leagues/day)
            DOW=$(date +%u)  # 1=Monday, 2=Tuesday, ...
            case $DOW in
              1) LEAGUES="premier_league la_liga eredivisie" ;;
              2) LEAGUES="serie_a bundesliga portuguese_liga" ;;
              3) LEAGUES="ligue_1 ekstraklasa belgian_pro_league scottish_premiership" ;;
              4) LEAGUES="turkish_super_lig liga_mx mls" ;;
              5) LEAGUES="niche_odds_only" ;;
              *) LEAGUES="${CONFIGURED_LEAGUES}"
                 echo "Not a scheduled collection day, processing all leagues" ;;
            esac
            echo "Auto-selected leagues for day $DOW: $LEAGUES"
          else
            LEAGUES="$LEAGUE_INPUT"
            echo "Processing single league: $LEAGUES"
          fi

          echo "leagues=$LEAGUES" >> $GITHUB_OUTPUT

      - name: Run match collector
        if: steps.leagues.outputs.leagues != 'niche_odds_only'
        id: collector
        run: |
          MODE="${{ github.event.inputs.collection_mode || 'update' }}"
          SEASON="${{ github.event.inputs.season || '2025' }}"
          START_SEASON="${{ github.event.inputs.start_season || '2023' }}"
          END_SEASON="${{ github.event.inputs.end_season || '2025' }}"
          DAYS_BACK="${{ github.event.inputs.days_back || '7' }}"
          INCLUDE_DETAILS="${{ github.event.inputs.include_details || 'true' }}"
          LEAGUES="${{ steps.leagues.outputs.leagues }}"

          DETAILS_FLAG=""
          if [ "$INCLUDE_DETAILS" = "true" ]; then
            DETAILS_FLAG="--include-all"
          fi

          echo "Collection mode: $MODE"
          echo "Leagues to process: $LEAGUES"

          for LEAGUE in $LEAGUES; do
            echo ""
            echo "========================================"
            echo "Collecting data for: $LEAGUE"
            echo "========================================"

            if [ "$MODE" = "bulk" ]; then
              echo "Collecting seasons $START_SEASON to $END_SEASON..."
              uv run python entrypoints/collect.py \
                --mode bulk \
                --league $LEAGUE \
                --start-season $START_SEASON \
                --end-season $END_SEASON \
                $DETAILS_FLAG || echo "::warning::Failed to collect $LEAGUE"
            elif [ "$MODE" = "season" ]; then
              echo "Collecting full season $SEASON..."
              uv run python entrypoints/collect.py \
                --mode season \
                --league $LEAGUE \
                --season $SEASON \
                $DETAILS_FLAG || echo "::warning::Failed to collect $LEAGUE"
            else
              echo "Smart update for season $SEASON, last $DAYS_BACK days..."
              uv run python entrypoints/collect.py \
                --mode update \
                --strategy smart \
                --league $LEAGUE \
                --season $SEASON \
                --days-back $DAYS_BACK || echo "::warning::Failed to collect $LEAGUE"
            fi
          done

      - name: Collect match statistics
        if: steps.leagues.outputs.leagues != 'niche_odds_only'
        run: |
          LEAGUES="${{ steps.leagues.outputs.leagues }}"
          BACKFILL="${{ github.event.inputs.backfill_cards || 'false' }}"
          BACKFILL_FLAG=""
          if [ "$BACKFILL" = "true" ]; then
            BACKFILL_FLAG="--backfill-cards"
            echo "Backfilling cards columns for: $LEAGUES"
          fi
          echo "Collecting match statistics for: $LEAGUES"
          uv run python scripts/collect_all_stats.py --leagues "$LEAGUES" $BACKFILL_FLAG 2>&1 | tail -50

      - name: Check for data changes
        if: steps.leagues.outputs.leagues != 'niche_odds_only'
        id: checksum_after
        run: |
          CHECKSUM=$(find data/01-raw -type f -name "*.parquet" -exec md5sum {} \; | sort | md5sum | cut -d' ' -f1)
          echo "checksum=$CHECKSUM" >> $GITHUB_OUTPUT
          echo "Raw data checksum after: $CHECKSUM"

          if [ "${{ steps.checksum_before.outputs.checksum }}" = "$CHECKSUM" ]; then
            echo "data_changed=false" >> $GITHUB_OUTPUT
            echo "No data changes detected"
          else
            echo "data_changed=true" >> $GITHUB_OUTPUT
            echo "Data changes detected"
          fi

      - name: Preprocess data for all leagues
        if: success() && steps.leagues.outputs.leagues != 'niche_odds_only' && (steps.checksum_after.outputs.data_changed == 'true' || github.event.inputs.skip_features != 'true')
        run: |
          LEAGUES="${{ steps.leagues.outputs.leagues }}"

          echo "Preprocessing data for leagues: $LEAGUES"

          for LEAGUE in $LEAGUES; do
            CONFIG_FILE="config/${LEAGUE}.yaml"

            if [ -f "$CONFIG_FILE" ]; then
              echo ""
              echo "========================================"
              echo "Preprocessing: $LEAGUE"
              echo "========================================"
              uv run python entrypoints/preprocess.py --config "$CONFIG_FILE" || echo "::warning::Failed to preprocess $LEAGUE"
            else
              echo "::warning::Config file not found for $LEAGUE: $CONFIG_FILE"
            fi
          done

      - name: Generate features for all leagues
        if: success() && steps.leagues.outputs.leagues != 'niche_odds_only' && (steps.checksum_after.outputs.data_changed == 'true' || github.event.inputs.skip_features != 'true')
        run: |
          LEAGUES="${{ steps.leagues.outputs.leagues }}"

          echo "Generating features for leagues: $LEAGUES"

          for LEAGUE in $LEAGUES; do
            CONFIG_FILE="config/${LEAGUE}.yaml"

            if [ -f "$CONFIG_FILE" ]; then
              echo ""
              echo "========================================"
              echo "Generating features: $LEAGUE"
              echo "========================================"
              uv run python entrypoints/features.py --config "$CONFIG_FILE" --output "features_${LEAGUE}.csv" || echo "::warning::Failed to generate features for $LEAGUE"
            else
              echo "::warning::Config file not found for $LEAGUE: $CONFIG_FILE"
            fi
          done

      - name: Fetch and merge betting odds
        if: success() && steps.leagues.outputs.leagues != 'niche_odds_only' && (steps.checksum_after.outputs.data_changed == 'true' || github.event.inputs.skip_features != 'true')
        run: |
          LEAGUES="${{ steps.leagues.outputs.leagues }}"
          SEASON="${{ github.event.inputs.season || '2025' }}"

          # Calculate seasons to fetch (current and previous for better coverage)
          CURRENT_YEAR=$((SEASON - 1))
          PREV_YEAR=$((CURRENT_YEAR - 1))

          echo "Fetching betting odds from football-data.co.uk..."
          echo "Seasons: $PREV_YEAR, $CURRENT_YEAR"

          for LEAGUE in $LEAGUES; do
            echo ""
            echo "========================================"
            echo "Fetching odds for: $LEAGUE"
            echo "========================================"

            uv run python entrypoints/fetch_odds.py \
              --league "$LEAGUE" \
              --seasons $PREV_YEAR $CURRENT_YEAR \
              --features-dir data/03-features \
              --cache-dir data/odds-cache \
              --output-suffix "_with_odds" || echo "::warning::Failed to fetch odds for $LEAGUE (non-critical)"
          done

      # Niche odds handled dynamically by "Spend remaining API budget" step at end.
      # Every day, after all other work, leftover requests go to niche odds.

      - name: Merge all league features
        if: success() && steps.leagues.outputs.leagues != 'niche_odds_only' && (steps.checksum_after.outputs.data_changed == 'true' || github.event.inputs.skip_features != 'true')
        run: |
          echo "Merging features from all leagues..."
          uv run python -c "
          import pandas as pd
          import numpy as np
          from pathlib import Path

          features_dir = Path('data/03-features')
          all_features = []

          # Use freshly generated feature files (prefer _with_odds if newer, else use base file)
          import os
          for league in ['premier_league', 'la_liga', 'serie_a', 'bundesliga', 'ligue_1', 'ekstraklasa', 'eredivisie', 'portuguese_liga', 'belgian_pro_league', 'scottish_premiership', 'turkish_super_lig', 'liga_mx', 'mls']:
              with_odds = features_dir / f'features_{league}_with_odds.csv'
              without_odds = features_dir / f'features_{league}.csv'

              # Choose the fresher file (or the one that exists)
              if with_odds.exists() and without_odds.exists():
                  # Use the more recently modified file
                  if os.path.getmtime(with_odds) >= os.path.getmtime(without_odds):
                      csv_file = with_odds
                      print(f'Loading {csv_file.name} (with odds, fresher)...')
                  else:
                      csv_file = without_odds
                      print(f'Loading {csv_file.name} (base file, fresher)...')
              elif with_odds.exists():
                  csv_file = with_odds
                  print(f'Loading {csv_file.name} (with odds)...')
              elif without_odds.exists():
                  csv_file = without_odds
                  print(f'Loading {csv_file.name} (no odds)...')
              else:
                  print(f'No features found for {league}')
                  continue

              df = pd.read_csv(csv_file)
              df['league'] = league
              all_features.append(df)
              print(f'  {len(df)} rows, {len(df.columns)} columns')

          if all_features:
              merged = pd.concat(all_features, ignore_index=True)

              # Derive home_goals/away_goals if missing (e.g. ekstraklasa)
              if 'total_goals' in merged.columns and 'goal_difference' in merged.columns:
                  if 'home_goals' not in merged.columns:
                      merged['home_goals'] = np.nan
                  if 'away_goals' not in merged.columns:
                      merged['away_goals'] = np.nan
                  mask = merged['home_goals'].isna() & merged['total_goals'].notna()
                  if mask.any():
                      merged.loc[mask, 'home_goals'] = ((merged.loc[mask, 'total_goals'] + merged.loc[mask, 'goal_difference']) / 2).astype(int)
                      merged.loc[mask, 'away_goals'] = ((merged.loc[mask, 'total_goals'] - merged.loc[mask, 'goal_difference']) / 2).astype(int)
                      print(f'Derived home_goals/away_goals for {mask.sum()} rows')

              # Fill btts (Both Teams To Score) target where missing
              if 'home_goals' in merged.columns and 'away_goals' in merged.columns:
                  btts_missing = merged['btts'].isna() if 'btts' in merged.columns else pd.Series(True, index=merged.index)
                  fill = btts_missing & merged['home_goals'].notna() & merged['away_goals'].notna()
                  if fill.any():
                      merged.loc[fill, 'btts'] = (
                          (merged.loc[fill, 'home_goals'] > 0) &
                          (merged.loc[fill, 'away_goals'] > 0)
                      ).astype(int)
                      print(f'Filled btts for {fill.sum()} rows from home_goals/away_goals')
              else:
                  print('WARNING: Cannot fill btts - missing home_goals/away_goals columns')

              # Save to multiple files for compatibility
              output_path = features_dir / 'features_all.csv'
              merged.to_csv(output_path, index=False)
              print(f'Merged features saved to {output_path}')

              # Save as features_all_5leagues_with_odds (both formats)
              odds_path = features_dir / 'features_all_5leagues_with_odds.csv'
              merged.to_csv(odds_path, index=False)
              merged.to_parquet(features_dir / 'features_all_5leagues_with_odds.parquet', index=False)
              print(f'Also saved to {odds_path} (.csv + .parquet)')

              # Also save as features_with_real_xg.csv (used by prediction script)
              xg_path = features_dir / 'features_with_real_xg.csv'
              merged.to_csv(xg_path, index=False)
              print(f'Also saved to {xg_path}')

              # Log btts column status for debugging
              if 'btts' in merged.columns:
                  btts_nan = merged['btts'].isna().sum()
                  print(f'btts column: {btts_nan} NaN values out of {len(merged)}')
              else:
                  print('WARNING: btts column not found in merged features!')

              print(f'Total: {len(merged)} rows, {len(merged.columns)} columns')
          else:
              print('No feature files found to merge')
          "

      - name: Upload all data to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 30
          max_attempts: 3
          retry_wait_seconds: 30
          command: |
            echo "Uploading all data to HF Hub..."
            uv run python entrypoints/upload_data.py

      - name: Spend remaining API budget on niche odds
        continue-on-error: true
        if: always()
        run: |
          uv run python -c "
          from src.data_collection.api_client import FootballAPIClient
          from src.odds.api_football_odds_loader import ApiFootballOddsLoader
          import logging
          logging.basicConfig(level=logging.INFO)

          client = FootballAPIClient()
          remaining = client.daily_limit - client.state.get('count', 0)
          print(f'API budget remaining: {remaining} requests')

          if remaining < 5:
              print('Not enough budget for niche odds, skipping')
              exit(0)

          from datetime import datetime
          dow = datetime.utcnow().weekday()
          all_leagues = ['premier_league', 'la_liga', 'serie_a', 'bundesliga', 'ligue_1', 'ekstraklasa', 'eredivisie', 'portuguese_liga', 'belgian_pro_league', 'scottish_premiership', 'turkish_super_lig', 'liga_mx', 'mls']
          start = (dow * 2) % len(all_leagues)
          leagues = all_leagues[start:] + all_leagues[:start]

          loader = ApiFootballOddsLoader(cache_dir='data/niche_odds_cache')

          for league in leagues:
              remaining = client.daily_limit - client.state.get('count', 0)
              if remaining < 5:
                  print(f'Budget exhausted ({remaining} left), stopping')
                  break

              max_fixtures = min((remaining - 1) // 4, 4)
              if max_fixtures < 1:
                  break

              print(f'Fetching niche odds for {league} (up to {max_fixtures} fixtures, {remaining} req left)...')
              fixture_ids = loader.get_upcoming_fixture_ids(league, next_n=max_fixtures)
              if fixture_ids:
                  df = loader.fetch_and_save(fixture_ids=fixture_ids)
                  if not df.empty:
                      print(f'  Got niche odds for {len(df)} fixtures')
                  else:
                      print(f'  No niche odds available')
              else:
                  print(f'  No upcoming fixtures')

          remaining = client.daily_limit - client.state.get('count', 0)
          print(f'Final budget: {remaining} requests remaining')
          " || echo "Could not run niche odds top-up"

      - name: Create summary
        if: always()
        run: |
          MODE="${{ github.event.inputs.collection_mode || 'update' }}"
          LEAGUES="${{ steps.leagues.outputs.leagues }}"
          SEASON="${{ github.event.inputs.season || '2025' }}"
          START_SEASON="${{ github.event.inputs.start_season || '2023' }}"
          END_SEASON="${{ github.event.inputs.end_season || '2025' }}"
          DATA_CHANGED="${{ steps.checksum_after.outputs.data_changed }}"

          echo "## Data Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ job.status }}" = "success" ]; then
            echo "**Pipeline completed successfully**" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Pipeline failed**" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Parameters" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Mode | \`$MODE\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Leagues | \`$LEAGUES\` |" >> $GITHUB_STEP_SUMMARY
          if [ "$MODE" = "bulk" ]; then
            echo "| Seasons | \`$START_SEASON - $END_SEASON\` |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Season | \`$SEASON\` |" >> $GITHUB_STEP_SUMMARY
          fi
          echo "| Data changed | \`$DATA_CHANGED\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Data Metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Raw data metrics per league
          for LEAGUE in $LEAGUES; do
            if [ -d "data/01-raw/$LEAGUE" ]; then
              echo "**$LEAGUE** (\`data/01-raw/$LEAGUE/\`):" >> $GITHUB_STEP_SUMMARY
              TOTAL_FIXTURES=0
              for season_dir in data/01-raw/$LEAGUE/*/; do
                if [ -d "$season_dir" ]; then
                  S=$(basename "$season_dir")
                  SIZE=$(du -sh "$season_dir" 2>/dev/null | cut -f1 || echo "N/A")
                  COUNT=$(uv run python -c "import pandas as pd; print(len(pd.read_parquet('$season_dir/matches.parquet')))" 2>/dev/null || echo "0")
                  echo "- Season \`$S\`: \`$COUNT\` fixtures (\`$SIZE\`)" >> $GITHUB_STEP_SUMMARY
                  TOTAL_FIXTURES=$((TOTAL_FIXTURES + COUNT))
                fi
              done
              echo "- **Total**: \`$TOTAL_FIXTURES\` fixtures" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          done

          # Merged features metrics
          if [ -f "data/03-features/features_all.csv" ]; then
            FEAT_SIZE=$(du -sh "data/03-features/features_all.csv" 2>/dev/null | cut -f1 || echo "N/A")
            FEAT_ROWS=$(wc -l < "data/03-features/features_all.csv" 2>/dev/null || echo "N/A")
            FEAT_COLS=$(head -1 "data/03-features/features_all.csv" 2>/dev/null | tr ',' '\n' | wc -l || echo "N/A")
            echo "**Merged Features** (\`features_all.csv\`):" >> $GITHUB_STEP_SUMMARY
            echo "- Size: \`$FEAT_SIZE\`" >> $GITHUB_STEP_SUMMARY
            echo "- Shape: \`$FEAT_ROWS rows x $FEAT_COLS columns\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # API usage
          if [ -f "state.json" ]; then
            API_COUNT=$(python3 -c "import json; print(json.load(open('state.json')).get('count', 'N/A'))" 2>/dev/null || echo "N/A")
            echo "### API Usage" >> $GITHUB_STEP_SUMMARY
            echo "- Requests today: \`$API_COUNT\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -d "data/odds-cache" ]; then
            ODDS_FILES=$(ls data/odds-cache/odds_*.csv 2>/dev/null | wc -l)
            if [ "$ODDS_FILES" -gt 0 ]; then
              echo "**Betting Odds** (\`data/odds-cache/\`):" >> $GITHUB_STEP_SUMMARY
              for odds_file in data/odds-cache/odds_*.csv; do
                if [ -f "$odds_file" ]; then
                  LEAGUE=$(basename "$odds_file" .csv | sed 's/odds_//')
                  ODDS_ROWS=$(wc -l < "$odds_file" 2>/dev/null || echo "0")
                  echo "- $LEAGUE: \`$ODDS_ROWS\` matches with odds" >> $GITHUB_STEP_SUMMARY
                fi
              done
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "### Pipeline Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. **Verify** - Secrets & API connectivity" >> $GITHUB_STEP_SUMMARY
          echo "2. **Download** - Sync from Hugging Face" >> $GITHUB_STEP_SUMMARY
          echo "3. **Collect** - Fetch data for each league" >> $GITHUB_STEP_SUMMARY
          echo "4. **Preprocess** - Raw -> Processed Parquet per league" >> $GITHUB_STEP_SUMMARY
          echo "5. **Features** - Generate ML features per league" >> $GITHUB_STEP_SUMMARY
          echo "6. **Odds** - Fetch betting odds from football-data.co.uk" >> $GITHUB_STEP_SUMMARY
          echo "7. **Merge** - Combine all league features (with odds)" >> $GITHUB_STEP_SUMMARY
          echo "8. **Upload** - Push all data to Hugging Face" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f fixtures_updater.log ]; then
            echo "### Recent Logs" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            tail -15 fixtures_updater.log >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: collector-logs
          path: |
            fixtures_updater.log
            state.json
          retention-days: 30
