name: Fetch SportMonks Historical Odds

on:
  workflow_dispatch:
    inputs:
      start_date:
        description: 'Start date (YYYY-MM-DD)'
        required: false
        default: '2022-08-01'
        type: string
      end_date:
        description: 'End date (YYYY-MM-DD)'
        required: false
        default: '2025-05-01'
        type: string
      markets:
        description: 'Markets to fetch'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - btts_only

permissions:
  contents: read

jobs:
  fetch-odds:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Verify secrets
        run: |
          if [ -z "${{ secrets.HF_TOKEN }}" ]; then
            echo "::error::HF_TOKEN secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.SPORTSMONK_KEY }}" ]; then
            echo "::error::SPORTSMONK_KEY secret is not set"
            exit 1
          fi
          echo "All required secrets are configured"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Cache UV dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Set up Python
        run: uv python install 3.10

      - name: Install dependencies
        run: uv sync --frozen

      - name: Create .env file
        run: |
          echo "SPORTSMONK_KEY=${{ secrets.SPORTSMONK_KEY }}" > .env
          echo "HF_TOKEN=${{ secrets.HF_TOKEN }}" >> .env

      - name: Create output directories
        run: |
          mkdir -p data/sportmonks_odds/raw
          mkdir -p data/sportmonks_odds/processed
          mkdir -p data/sportmonks_odds/processed_existing
          mkdir -p data/sportmonks_odds/merged
          mkdir -p data/03-features

      - name: Download existing processed odds from HF (for merging)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python -c "
          from huggingface_hub import hf_hub_download, list_repo_files
          import os
          import shutil

          token = os.getenv('HF_TOKEN')

          # List files in repo
          try:
              files = list_repo_files(
                  repo_id='czlowiekZplanety/bettip-data',
                  repo_type='dataset',
                  token=token
              )

              # Download existing processed odds files
              processed_files = [f for f in files if 'sportmonks_odds/processed/' in f and f.endswith('.csv')]
              print(f'Found {len(processed_files)} existing processed odds files')

              for f in processed_files:
                  print(f'Downloading {f}...')
                  path = hf_hub_download(
                      repo_id='czlowiekZplanety/bettip-data',
                      repo_type='dataset',
                      filename=f,
                      token=token
                  )
                  # Copy to existing dir for later merge
                  dest = f'data/sportmonks_odds/processed_existing/{os.path.basename(f)}'
                  shutil.copy(path, dest)
                  print(f'  Saved to {dest}')

          except Exception as e:
              print(f'Warning: Could not download existing files: {e}')
              print('Will create new files without merging')
          "

      - name: Download features file from HF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python -c "
          from huggingface_hub import hf_hub_download
          import os

          token = os.getenv('HF_TOKEN')
          hf_hub_download(
              repo_id='czlowiekZplanety/bettip-data',
              repo_type='dataset',
              filename='data/03-features/features_all_5leagues_with_odds.csv',
              local_dir='.',
              token=token
          )
          print('Downloaded features_all_5leagues_with_odds.csv')
          "

      - name: Check SportMonks API subscription
        run: |
          uv run python -c "
          import os
          import requests
          from dotenv import load_dotenv
          load_dotenv()

          token = os.getenv('SPORTSMONK_KEY')
          resp = requests.get(
              'https://api.sportmonks.com/v3/my/subscriptions',
              params={'api_token': token}
          )
          data = resp.json()
          print('SportMonks Subscription:')
          for sub in data.get('data', []):
              meta = sub.get('meta', {})
              print(f'  Trial ends: {meta.get(\"trial_ends_at\", \"N/A\")}')
              print(f'  Plan ends: {meta.get(\"ends_at\", \"N/A\")}')
              for plan in sub.get('plans', []):
                  print(f'  Plan: {plan.get(\"plan\")} ({plan.get(\"category\")})')
          "

      - name: Fetch historical odds from SportMonks
        run: |
          START_DATE="${{ github.event.inputs.start_date || '2022-08-01' }}"
          END_DATE="${{ github.event.inputs.end_date || '2025-05-01' }}"

          echo "Fetching odds from $START_DATE to $END_DATE"

          uv run python scripts/sportmonks/bulk_fetch_historical_odds.py \
            --start-date "$START_DATE" \
            --end-date "$END_DATE"

      - name: Show fetch results
        run: |
          echo "=== Fetch Results ==="

          if [ -f "data/sportmonks_odds/fetch_checkpoint.json" ]; then
            cat data/sportmonks_odds/fetch_checkpoint.json
          fi

          echo ""
          echo "=== Raw files ==="
          ls -lh data/sportmonks_odds/raw/ || true

          echo ""
          echo "=== Processed files (new) ==="
          ls -lh data/sportmonks_odds/processed/ || true

      - name: Merge new data with existing data
        run: |
          uv run python -c "
          import pandas as pd
          from pathlib import Path
          import os

          processed_dir = Path('data/sportmonks_odds/processed')
          existing_dir = Path('data/sportmonks_odds/processed_existing')

          print('=== Merging new data with existing data ===')

          for market in ['btts_odds', 'corners_odds', 'cards_odds', 'shots_odds']:
              new_file = processed_dir / f'{market}.csv'
              existing_file = existing_dir / f'{market}.csv'

              if new_file.exists() and existing_file.exists():
                  print(f'\n{market}:')
                  new_df = pd.read_csv(new_file)
                  existing_df = pd.read_csv(existing_file)
                  print(f'  New data: {len(new_df):,} rows')
                  print(f'  Existing data: {len(existing_df):,} rows')

                  # Merge and deduplicate by fixture_id
                  merged = pd.concat([existing_df, new_df], ignore_index=True)
                  merged = merged.drop_duplicates(subset=['fixture_id'], keep='last')
                  print(f'  Merged (deduped): {len(merged):,} rows')

                  # Save back
                  merged.to_csv(new_file, index=False)
                  print(f'  Saved merged data')

              elif new_file.exists():
                  print(f'{market}: Only new data ({len(pd.read_csv(new_file)):,} rows)')
              elif existing_file.exists():
                  print(f'{market}: Only existing data, copying...')
                  import shutil
                  shutil.copy(existing_file, new_file)
              else:
                  print(f'{market}: No data found')

          print('\n=== Merge complete ===')
          "

      - name: Validate fetched data
        run: |
          uv run python -c "
          import json
          import sys

          # Check validation report
          try:
              with open('data/sportmonks_odds/validation_report.json') as f:
                  results = json.load(f)

              print('=== DATA VALIDATION REPORT ===')

              has_errors = False
              has_warnings = False

              for r in results:
                  market = r['market']
                  total = r['total_rows']
                  fixtures = r.get('unique_fixtures', 0)
                  nan_ratio = r.get('nan_ratio', 0) * 100

                  status = '✅'
                  if r.get('errors'):
                      status = '❌'
                      has_errors = True
                  elif r.get('warnings'):
                      status = '⚠️'
                      has_warnings = True

                  print(f'{status} {market}: {total:,} rows, {fixtures:,} fixtures, {nan_ratio:.1f}% NaN')

                  if r.get('warnings'):
                      for w in r['warnings']:
                          print(f'   ⚠️ {w}')
                  if r.get('errors'):
                      for e in r['errors']:
                          print(f'   ❌ {e}')

              print()

              # Check checkpoint
              with open('data/sportmonks_odds/fetch_checkpoint.json') as f:
                  checkpoint = json.load(f)

              if checkpoint.get('completed'):
                  print('✅ Fetch completed successfully')
              else:
                  print('⚠️ Fetch incomplete - may need to resume')
                  has_warnings = True

              print(f'   Total fixtures: {checkpoint.get(\"total_fixtures\", 0):,}')
              print(f'   Total odds entries: {checkpoint.get(\"total_odds_entries\", 0):,}')

              if has_errors:
                  print()
                  print('❌ VALIDATION FAILED - Data has critical errors')
                  sys.exit(1)
              elif has_warnings:
                  print()
                  print('⚠️ Validation passed with warnings')
              else:
                  print()
                  print('✅ Validation passed')

          except FileNotFoundError as e:
              print(f'❌ Validation file not found: {e}')
              sys.exit(1)
          "

      - name: Download features file from HF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python -c "
          from huggingface_hub import hf_hub_download
          import os
          import shutil

          token = os.getenv('HF_TOKEN')

          # Download the features file specifically
          local_path = hf_hub_download(
              repo_id='czlowiekZplanety/bettip-data',
              repo_type='dataset',
              filename='data/03-features/features_all_5leagues_with_odds.csv',
              token=token
          )

          # Copy to expected location
          dest = 'data/03-features/features_all_5leagues_with_odds.csv'
          os.makedirs(os.path.dirname(dest), exist_ok=True)
          shutil.copy(local_path, dest)

          # Verify
          if os.path.exists(dest):
              lines = sum(1 for _ in open(dest))
              print(f'✅ Downloaded features file: {lines:,} lines')
          else:
              print('❌ Failed to download features file')
              exit(1)
          "

      - name: Merge odds with features
        run: |
          uv run python scripts/sportmonks/merge_sportmonks_odds.py

      - name: Download existing features with odds from HF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python -c "
          from huggingface_hub import hf_hub_download
          import os
          import shutil

          token = os.getenv('HF_TOKEN')

          try:
              # Download existing features with SportMonks odds
              local_path = hf_hub_download(
                  repo_id='czlowiekZplanety/bettip-data',
                  repo_type='dataset',
                  filename='data/03-features/features_with_sportmonks_odds.csv',
                  token=token
              )

              # Copy to a known location for combining
              os.makedirs('data/sportmonks_odds/existing', exist_ok=True)
              shutil.copy(local_path, 'data/sportmonks_odds/existing/features_with_sportmonks_odds.csv')
              print('✅ Downloaded existing features_with_sportmonks_odds.csv')

          except Exception as e:
              print(f'⚠️ Could not download existing file (may not exist yet): {e}')
          "

      - name: Combine new merge with existing odds
        run: |
          uv run python -c "
          import pandas as pd
          import numpy as np
          import os

          existing_path = 'data/sportmonks_odds/existing/features_with_sportmonks_odds.csv'
          fresh_merge_path = 'data/sportmonks_odds/merged/features_with_sportmonks_odds.csv'

          if not os.path.exists(existing_path):
              print('No existing file to combine with, using fresh merge only')
              exit(0)

          print('Combining fresh merge with existing odds data...')

          existing = pd.read_csv(existing_path, low_memory=False)
          fresh = pd.read_csv(fresh_merge_path, low_memory=False)

          print(f'Existing rows: {len(existing):,}')
          print(f'Fresh merge rows: {len(fresh):,}')

          # Create merge key
          fresh['key'] = fresh['date'].astype(str) + '_' + fresh['home_team_name'].astype(str) + '_' + fresh['away_team_name'].astype(str)
          existing['key'] = existing['date'].astype(str) + '_' + existing['home_team_name'].astype(str) + '_' + existing['away_team_name'].astype(str)

          # Index existing for fast lookup
          existing_indexed = existing.set_index('key')

          # Odds columns to fill
          odds_cols = ['sm_btts_yes_odds', 'sm_btts_no_odds',
                       'sm_corners_over_odds', 'sm_corners_under_odds', 'sm_corners_line',
                       'sm_cards_over_odds', 'sm_cards_under_odds', 'sm_cards_line',
                       'sm_shots_over_odds', 'sm_shots_under_odds', 'sm_shots_line',
                       'sm_fixture_id']

          filled = {col: 0 for col in odds_cols}

          # Fill missing values in fresh from existing
          for idx, row in fresh.iterrows():
              key = row['key']
              if key in existing_indexed.index:
                  existing_row = existing_indexed.loc[key]
                  if isinstance(existing_row, pd.DataFrame):
                      existing_row = existing_row.iloc[0]

                  for col in odds_cols:
                      if col in fresh.columns and col in existing_indexed.columns:
                          if pd.isna(row.get(col)) and pd.notna(existing_row.get(col)):
                              fresh.loc[idx, col] = existing_row[col]
                              filled[col] += 1

          # Remove temp column and save
          fresh = fresh.drop(columns=['key'])
          fresh.to_csv(fresh_merge_path, index=False)

          print('Filled from existing:')
          for col, count in filled.items():
              if count > 0:
                  print(f'  {col}: {count}')

          print()
          print('=== COMBINED COVERAGE ===')
          print(f'BTTS: {fresh[\"sm_btts_yes_odds\"].notna().sum():,}')
          print(f'Corners: {fresh[\"sm_corners_over_odds\"].notna().sum():,}')
          print(f'Cards: {fresh[\"sm_cards_over_odds\"].notna().sum():,}')
          print(f'Shots: {fresh[\"sm_shots_over_odds\"].notna().sum():,}')
          "

      - name: Copy merged file to data directory
        run: |
          cp data/sportmonks_odds/merged/features_with_sportmonks_odds.csv \
             data/03-features/features_with_sportmonks_odds.csv

          echo "Merged file stats:"
          wc -l data/03-features/features_with_sportmonks_odds.csv

          # Also create Parquet version for new pipeline
          uv run python -c "
          import pandas as pd
          df = pd.read_csv('data/03-features/features_with_sportmonks_odds.csv', low_memory=False)
          df.to_parquet('data/03-features/features_with_sportmonks_odds.parquet', index=False)
          print(f'Created Parquet version: {len(df)} rows')

          total = len(df)
          corners = df['sm_corners_over_odds'].notna().sum()
          cards = df['sm_cards_over_odds'].notna().sum()
          btts = df['sm_btts_yes_odds'].notna().sum() if 'sm_btts_yes_odds' in df.columns else 0

          print(f'Total fixtures: {total}')
          print(f'Corners coverage: {corners}/{total} ({corners/total*100:.1f}%)')
          print(f'Cards coverage: {cards}/{total} ({cards/total*100:.1f}%)')
          print(f'BTTS coverage: {btts}/{total} ({btts/total*100:.1f}%)')
          "

      - name: Upload to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python -c "
          from huggingface_hub import HfApi
          import os

          api = HfApi(token=os.getenv('HF_TOKEN'))

          # Upload merged features with sportmonks odds (both formats)
          for ext in ['csv', 'parquet']:
              fpath = f'data/03-features/features_with_sportmonks_odds.{ext}'
              if os.path.exists(fpath):
                  api.upload_file(
                      path_or_fileobj=fpath,
                      path_in_repo=fpath,
                      repo_id='czlowiekZplanety/bettip-data',
                      repo_type='dataset',
                      commit_message=f'Update features with SportMonks historical odds (.{ext})'
                  )
                  print(f'Uploaded features_with_sportmonks_odds.{ext} to HF Hub')

          # Also upload raw odds files for future use
          import os
          raw_dir = 'data/sportmonks_odds/raw'
          for fname in os.listdir(raw_dir):
              if fname.endswith('.csv'):
                  api.upload_file(
                      path_or_fileobj=f'{raw_dir}/{fname}',
                      path_in_repo=f'data/sportmonks_odds/raw/{fname}',
                      repo_id='czlowiekZplanety/bettip-data',
                      repo_type='dataset',
                      commit_message=f'Upload SportMonks raw odds: {fname}'
                  )
                  print(f'Uploaded {fname}')

          # Upload processed odds
          proc_dir = 'data/sportmonks_odds/processed'
          for fname in os.listdir(proc_dir):
              if fname.endswith('.csv'):
                  api.upload_file(
                      path_or_fileobj=f'{proc_dir}/{fname}',
                      path_in_repo=f'data/sportmonks_odds/processed/{fname}',
                      repo_id='czlowiekZplanety/bettip-data',
                      repo_type='dataset',
                      commit_message=f'Upload SportMonks processed odds: {fname}'
                  )
                  print(f'Uploaded {fname}')
          "

      - name: Create summary
        if: always()
        run: |
          echo "## SportMonks Odds Fetch Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          START_DATE="${{ github.event.inputs.start_date || '2022-08-01' }}"
          END_DATE="${{ github.event.inputs.end_date || '2025-05-01' }}"

          echo "### Parameters" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Start Date | \`$START_DATE\` |" >> $GITHUB_STEP_SUMMARY
          echo "| End Date | \`$END_DATE\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "data/sportmonks_odds/fetch_checkpoint.json" ]; then
            echo "### Fetch Results" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat data/sportmonks_odds/fetch_checkpoint.json >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Files Generated" >> $GITHUB_STEP_SUMMARY
          echo "| File | Size |" >> $GITHUB_STEP_SUMMARY
          echo "|------|------|" >> $GITHUB_STEP_SUMMARY

          for f in data/sportmonks_odds/raw/*.csv; do
            if [ -f "$f" ]; then
              SIZE=$(du -h "$f" | cut -f1)
              NAME=$(basename "$f")
              echo "| \`raw/$NAME\` | $SIZE |" >> $GITHUB_STEP_SUMMARY
            fi
          done

          for f in data/sportmonks_odds/processed/*.csv; do
            if [ -f "$f" ]; then
              SIZE=$(du -h "$f" | cut -f1)
              NAME=$(basename "$f")
              echo "| \`processed/$NAME\` | $SIZE |" >> $GITHUB_STEP_SUMMARY
            fi
          done

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sportmonks-odds-data
          path: |
            data/sportmonks_odds/
          retention-days: 30
