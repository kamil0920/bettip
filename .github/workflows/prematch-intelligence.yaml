name: Pre-Match Intelligence
# Unified workflow: combines schedule fetching, ML predictions, CLV tracking, and lineup collection

on:
  schedule:
    - cron: '0 7 * * 5,6,0'     # 7 AM UTC: Morning predictions (Fri, Sat, Sun only)
    - cron: '30 9-23 * * 5,6,0' # Hourly 9:30-23:30 UTC: Lineup checks (Fri, Sat, Sun only)

  workflow_dispatch:
    inputs:
      leagues:
        description: 'Leagues to monitor (space-separated, or "all")'
        required: false
        default: 'all'
        type: string
      notify:
        description: 'Send Telegram notifications'
        required: false
        default: true
        type: boolean

permissions:
  contents: write

env:
  ALL_LEAGUES: "bundesliga ekstraklasa la_liga ligue_1 premier_league serie_a"
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_REPO_ID: ${{ vars.HF_REPO_ID }}

jobs:
  # ==========================================================================
  # JOB 1: Morning predictions (7 AM UTC)
  # - Fetch schedule from API-Football
  # - Update previous results (CLV tracking)
  # - Run ML predictions
  # - Import to CLV tracker
  # - Send Telegram summary
  # ==========================================================================
  morning-predictions:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'schedule' && github.event.schedule == '0 7 * * *')

    steps:
      - name: Setup Python environment
        uses: ./.github/actions/setup-python-uv

      - name: Create .env file
        run: |
          echo "API_FOOTBALL_KEY=${{ secrets.API_FOOTBALL_KEY }}" > .env
          echo "HF_TOKEN=${{ secrets.HF_TOKEN }}" >> .env
          echo "DAILY_LIMIT=100" >> .env
          echo "PER_MIN_LIMIT=10" >> .env

      - name: Download data from HF Hub
        run: |
          echo "Downloading features and models..."
          uv run python entrypoints/download_data.py

      - name: Download sniper deployment config
        run: |
          echo "Downloading sniper deployment config..."
          uv run python -c "
          from huggingface_hub import hf_hub_download
          import os
          from pathlib import Path

          try:
              path = hf_hub_download(
                  repo_id=os.getenv('HF_REPO_ID', 'czlowiekZplanety/bettip-data'),
                  filename='config/sniper_deployment.json',
                  repo_type='dataset',
                  token=os.getenv('HF_TOKEN'),
                  local_dir='.'
              )
              print(f'Downloaded: {path}')
          except Exception as e:
              print(f'Warning: Could not download sniper config: {e}')
              print('Using default strategies.yaml')
          " || echo "Using strategies.yaml fallback"

      - name: Update previous results (CLV tracking)
        run: |
          echo "Updating results for previous predictions..."
          uv run python experiments/update_results.py 2>&1 || echo "Results update skipped (no previous predictions)"

      - name: Determine leagues
        id: leagues
        run: |
          LEAGUE_INPUT="${{ github.event.inputs.leagues || 'all' }}"
          if [ "$LEAGUE_INPUT" = "all" ]; then
            LEAGUES="${ALL_LEAGUES}"
          else
            LEAGUES="$LEAGUE_INPUT"
          fi
          echo "leagues=$LEAGUES" >> $GITHUB_OUTPUT

      - name: Fetch daily schedule (local parquet, 0 API calls)
        run: |
          echo "Loading today's schedule from local parquet files..."
          uv run python -m src.data_collection.match_scheduler --fetch --days-ahead 0 --local --leagues ${{ steps.leagues.outputs.leagues }}

      - name: Run ML predictions (local models only, 0 API calls)
        id: predict
        run: |
          echo "Running ML predictions (local only, saving API budget for pre-kickoff)..."
          uv run python -m src.data_collection.match_scheduler --predict --edge 0.05 --skip-api

          # Get counts
          if [ -f data/06-prematch/today_schedule.json ]; then
            TOTAL=$(uv run python -c "import json; print(json.load(open('data/06-prematch/today_schedule.json'))['total_matches'])")
            echo "total_matches=$TOTAL" >> $GITHUB_OUTPUT
          else
            echo "total_matches=0" >> $GITHUB_OUTPUT
          fi

          if [ -f data/06-prematch/interesting_matches.json ]; then
            INTERESTING=$(uv run python -c "import json; print(json.load(open('data/06-prematch/interesting_matches.json'))['interesting_count'])")
            echo "interesting_count=$INTERESTING" >> $GITHUB_OUTPUT
          else
            echo "interesting_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Generate recommendations CSV
        id: recommendations
        run: |
          echo "Generating recommendations..."
          uv run python experiments/generate_daily_recommendations.py --min-edge 5 2>&1 | tee predictions_log.txt || true

          # Find generated file
          DATE=$(date +%Y%m%d)
          REC_FILE=$(ls -t data/05-recommendations/rec_${DATE}_*.csv 2>/dev/null | head -1)

          if [ -n "$REC_FILE" ]; then
            COUNT=$(wc -l < "$REC_FILE")
            echo "predictions_count=$((COUNT - 1))" >> $GITHUB_OUTPUT
            echo "predictions_file=$REC_FILE" >> $GITHUB_OUTPUT
          else
            echo "predictions_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Import to CLV tracker
        if: steps.recommendations.outputs.predictions_count > 0
        run: |
          REC_FILE="${{ steps.recommendations.outputs.predictions_file }}"
          if [ -n "$REC_FILE" ] && [ -f "$REC_FILE" ]; then
            uv run python experiments/clv_workflow.py import --file "$REC_FILE" 2>&1 || true
          fi

      # Niche odds dropped from weekends — already fetched Mon-Thu.
      # Saves ~34 req for hourly lineup collection checks.

      - name: Upload prematch data to HF Hub
        continue-on-error: true
        run: |
          echo "Uploading prematch data to HF Hub..."
          uv run python -c "
          import os
          from huggingface_hub import HfApi
          from pathlib import Path

          api = HfApi(token=os.getenv('HF_TOKEN'))
          repo_id = 'czlowiekZplanety/bettip-data'

          for folder in ['data/05-recommendations', 'data/06-prematch']:
              if Path(folder).exists():
                  api.upload_folder(
                      folder_path=folder,
                      path_in_repo=folder,
                      repo_id=repo_id,
                      repo_type='dataset',
                      commit_message=f'Update {folder} - $(date +%Y-%m-%d)'
                  )
                  print(f'Uploaded {folder}')
          "

      - name: Send Telegram summary
        continue-on-error: true
        if: steps.predict.outputs.interesting_count > 0
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          if [ -n "$TELEGRAM_BOT_TOKEN" ] && [ -n "$TELEGRAM_CHAT_ID" ]; then
            uv run python << 'PYEOF'
          import json
          import os
          import requests

          with open('data/06-prematch/interesting_matches.json') as f:
              data = json.load(f)

          lines = ["*DAILY PREDICTIONS*", ""]
          lines.append(f"Total matches: {data['total_matches']}")
          lines.append(f"Interesting: {data['interesting_count']}")
          lines.append("")

          for m in data['matches'][:10]:
              pred = m.get('prediction', {})
              edge = pred.get('max_edge', 0) * 100
              market = pred.get('best_market', 'N/A')
              lines.append(f"*{m['home_team']} vs {m['away_team']}*")
              lines.append(f"  {edge:.1f}% edge on {market}")
              lines.append("")

          message = "\n".join(lines)
          token = os.environ.get('TELEGRAM_BOT_TOKEN')
          chat_id = os.environ.get('TELEGRAM_CHAT_ID')

          if token and chat_id:
              requests.post(
                  f"https://api.telegram.org/bot{token}/sendMessage",
                  data={'chat_id': chat_id, 'text': message, 'parse_mode': 'Markdown'}
              )
              print("Telegram notification sent!")
          PYEOF
          fi

      - name: Create summary
        if: always()
        run: |
          TOTAL="${{ steps.predict.outputs.total_matches || '0' }}"
          INTERESTING="${{ steps.predict.outputs.interesting_count || '0' }}"
          RECS="${{ steps.recommendations.outputs.predictions_count || '0' }}"

          echo "## Daily Predictions - $(date +%Y-%m-%d)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total matches | $TOTAL |" >> $GITHUB_STEP_SUMMARY
          echo "| Interesting (edge >= 5%) | $INTERESTING |" >> $GITHUB_STEP_SUMMARY
          echo "| Recommendations generated | $RECS |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f data/06-prematch/interesting_matches.json ]; then
            echo "### Top Predictions" >> $GITHUB_STEP_SUMMARY
            uv run python -c "
          import json
          with open('data/06-prematch/interesting_matches.json') as f:
              data = json.load(f)
          for m in data['matches'][:10]:
              pred = m.get('prediction', {})
              edge = pred.get('max_edge', 0) * 100
              market = pred.get('best_market', 'N/A')
              print(f'- **{m[\"home_team\"]} vs {m[\"away_team\"]}**: {edge:.1f}% edge on {market}')
          " >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: daily-predictions-${{ github.run_number }}
          path: |
            data/06-prematch/
            data/05-recommendations/
            predictions_log.txt
          retention-days: 30

  # ==========================================================================
  # JOB 2: Lineup collection (every hour at :30)
  # - Check local schedule for matches 40-50 mins away
  # - Collect lineups for interesting matches only
  # - Send Telegram alert
  # ==========================================================================
  collect-lineups:
    runs-on: ubuntu-latest
    # Run on :30 schedule (not 7 AM). Check minute to handle cron string variations.
    if: github.event_name == 'schedule' && github.event.schedule != '0 7 * * *'

    steps:
      - name: Setup Python environment
        uses: ./.github/actions/setup-python-uv

      - name: Create .env file
        run: |
          echo "API_FOOTBALL_KEY=${{ secrets.API_FOOTBALL_KEY }}" > .env
          echo "HF_TOKEN=${{ secrets.HF_TOKEN }}" >> .env
          echo "DAILY_LIMIT=100" >> .env
          echo "PER_MIN_LIMIT=10" >> .env

      - name: Download prematch data from HF Hub
        run: |
          echo "Downloading prematch schedule from HF Hub..."
          uv run python -c "
          import os
          from huggingface_hub import snapshot_download

          snapshot_download(
              repo_id=os.getenv('HF_REPO_ID', 'czlowiekZplanety/bettip-data'),
              repo_type='dataset',
              local_dir='.',
              allow_patterns=['data/06-prematch/**'],
              token=os.getenv('HF_TOKEN'),
              force_download=True
          )
          print('Downloaded prematch data')
          " || echo "No prematch data in HF yet"

      - name: Check if collection needed
        id: check
        run: |
          uv run python -c "
          import json
          import sys
          from datetime import datetime, timezone, timedelta
          from pathlib import Path

          schedule_file = Path('data/06-prematch/today_schedule.json')
          interesting_file = Path('data/06-prematch/interesting_matches.json')

          if not schedule_file.exists():
              print('No schedule file - skipping')
              sys.exit(0)

          with open(schedule_file) as f:
              schedule = json.load(f)

          interesting_ids = set()
          if interesting_file.exists():
              with open(interesting_file) as f:
                  interesting = json.load(f)
              interesting_ids = {m['fixture_id'] for m in interesting.get('matches', [])}
              print(f'Loaded {len(interesting_ids)} interesting matches')

          now = datetime.now(timezone.utc)
          # Window 15-70 min catches ALL kickoff times (:00, :15, :30, :45)
          # With workflow at :30: catches :45 to :40 of next hour (55 min span)
          # Example: 18:30 workflow catches 18:45-19:40
          # Example: 19:30 workflow catches 19:45-20:40 (catches :45 kickoffs)
          window_start = now + timedelta(minutes=15)
          window_end = now + timedelta(minutes=70)

          matches_in_window = []
          for m in schedule.get('matches', []):
              kickoff = datetime.fromisoformat(m['kickoff'])
              if window_start <= kickoff <= window_end:
                  if not interesting_ids or m['fixture_id'] in interesting_ids:
                      mins = int((kickoff - now).total_seconds() / 60)
                      m['mins_until'] = mins
                      matches_in_window.append(m)

          if matches_in_window:
              print(f'COLLECT NOW! {len(matches_in_window)} match(es):')
              for m in matches_in_window:
                  print(f'  {m[\"home_team\"]} vs {m[\"away_team\"]} in {m[\"mins_until\"]} mins')
              with open('matches_to_collect.json', 'w') as f:
                  json.dump(matches_in_window, f)
          else:
              print('No interesting matches in lineup window')
          "

          if [ -f matches_to_collect.json ]; then
            echo "should_collect=true" >> $GITHUB_OUTPUT
          else
            echo "should_collect=false" >> $GITHUB_OUTPUT
          fi

      - name: Collect lineups
        if: steps.check.outputs.should_collect == 'true'
        id: collect
        run: |
          echo "Collecting lineup data (budget-aware, highest edge first)..."
          uv run python -m src.data_collection.match_scheduler --collect --budget-aware

          # Check if collection produced results
          if [ -f data/06-prematch/lineup_collection.json ]; then
            if uv run python -c "import json; r=json.load(open('data/06-prematch/lineup_collection.json')); exit(0 if any(m['signals'] for m in r['recommendations']) else 1)"; then
              echo "has_signals=true" >> $GITHUB_OUTPUT
            else
              echo "has_signals=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "No lineup collection file created"
            echo "has_signals=false" >> $GITHUB_OUTPUT
          fi

      - name: Send Telegram alert (once per match)
        continue-on-error: true
        if: steps.check.outputs.should_collect == 'true' && steps.collect.outputs.has_signals == 'true'
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          uv run python << 'PYEOF'
          import json
          import os
          import requests
          from pathlib import Path
          from datetime import datetime, timezone

          NOTIFIED_FILE = Path('data/06-prematch/notified_fixtures.json')

          # Load already-notified fixtures
          notified = set()
          if NOTIFIED_FILE.exists():
              try:
                  data = json.load(open(NOTIFIED_FILE))
                  notified = set(data.get('fixture_ids', []))
                  print(f"Loaded {len(notified)} previously notified fixtures")
              except:
                  pass

          with open('data/06-prematch/lineup_collection.json') as f:
              results = json.load(f)

          lines = ["*LINEUP ALERT*", ""]
          new_notifications = []

          for rec in results['recommendations']:
              if not rec.get('signals'):
                  continue

              fixture_id = rec.get('fixture_id')
              if fixture_id in notified:
                  print(f"Skipping already notified: {rec['match']}")
                  continue

              new_notifications.append(fixture_id)
              lines.append(f"*{rec['match']}*")
              lines.append(f"Kickoff in {rec['mins_until']} mins")
              lines.append(f"{rec.get('home_formation', '?')} vs {rec.get('away_formation', '?')}")

              weather = rec.get('weather')
              if weather:
                  temp = weather.get('temperature', 0)
                  wind = weather.get('wind_speed', 0)
                  lines.append(f"{weather.get('conditions', '')}, {temp:.0f}C, {wind:.0f}km/h")

              lines.append("")
              for sig in rec['signals']:
                  lines.append(f"*{sig['market']}* ({sig['confidence']:.0%})")
                  lines.append(f"_{sig['reason']}_")
              lines.append("")

          if new_notifications and len(lines) > 2:
              message = "\n".join(lines)
              token = os.environ.get('TELEGRAM_BOT_TOKEN')
              chat_id = os.environ.get('TELEGRAM_CHAT_ID')

              if token and chat_id:
                  requests.post(
                      f"https://api.telegram.org/bot{token}/sendMessage",
                      data={'chat_id': chat_id, 'text': message, 'parse_mode': 'Markdown'}
                  )
                  print(f"Telegram notification sent for {len(new_notifications)} match(es)!")

              # Save notified fixtures
              notified.update(new_notifications)
              NOTIFIED_FILE.parent.mkdir(parents=True, exist_ok=True)
              with open(NOTIFIED_FILE, 'w') as f:
                  json.dump({
                      'updated_at': datetime.now(timezone.utc).isoformat(),
                      'fixture_ids': list(notified)
                  }, f)
              print(f"Saved {len(notified)} notified fixtures")
          else:
              print("No new matches to notify")
          PYEOF

      # No niche odds on weekends — budget reserved for lineup checks.
      # Niche odds covered Mon-Thu by collect-match-data workflow.

      - name: Upload notified fixtures to HF Hub
        continue-on-error: true
        if: steps.check.outputs.should_collect == 'true'
        run: |
          uv run python -c "
          import os
          from huggingface_hub import HfApi
          from pathlib import Path

          notified_file = Path('data/06-prematch/notified_fixtures.json')
          if notified_file.exists():
              api = HfApi(token=os.getenv('HF_TOKEN'))
              api.upload_file(
                  path_or_fileobj=str(notified_file),
                  path_in_repo='data/06-prematch/notified_fixtures.json',
                  repo_id=os.getenv('HF_REPO_ID', 'czlowiekZplanety/bettip-data'),
                  repo_type='dataset',
                  commit_message='Update notified fixtures'
              )
              print('Uploaded notified fixtures to HF Hub')
          "
