name: Pre-Match Intelligence
# Unified workflow: combines schedule fetching, ML predictions, CLV tracking, and lineup collection

on:
  schedule:
    - cron: '0 7 * * 5,6,0'     # 7 AM UTC: Morning predictions (Fri, Sat, Sun only)
    - cron: '30 11-20 * * 5,6,0' # Hourly 11:30-20:30 UTC: Pre-kickoff lineup + re-prediction

  workflow_dispatch:
    inputs:
      leagues:
        description: 'Leagues to monitor (space-separated, or "all")'
        required: false
        default: 'all'
        type: string
      notify:
        description: 'Send Telegram notifications'
        required: false
        default: true
        type: boolean

permissions:
  contents: write

env:
  ALL_LEAGUES: "belgian_pro_league bundesliga eredivisie la_liga ligue_1 portuguese_liga premier_league scottish_premiership serie_a turkish_super_lig"
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_REPO_ID: ${{ vars.HF_REPO_ID }}

jobs:
  # ==========================================================================
  # JOB 1: Morning predictions (7 AM UTC)
  # - Fetch schedule from API-Football
  # - Update previous results (CLV tracking)
  # - Run ML predictions
  # - Import to CLV tracker
  # - Send Telegram summary
  # ==========================================================================
  morning-predictions:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'schedule' && github.event.schedule == '0 7 * * 5,6,0')

    steps:
      - uses: actions/checkout@v4
      - name: Setup Python environment
        uses: ./.github/actions/setup-python-uv
        with:
          extras: dl

      - name: Create .env file
        run: |
          echo "API_FOOTBALL_KEY=${{ secrets.API_FOOTBALL_KEY }}" > .env
          echo "HF_TOKEN=${{ secrets.HF_TOKEN }}" >> .env
          echo "DAILY_LIMIT=500" >> .env
          echo "PER_MIN_LIMIT=30" >> .env

      - name: Download data from HF Hub
        run: |
          echo "Downloading features and models..."
          uv run python entrypoints/download_data.py

      - name: Download sniper deployment config
        run: |
          echo "Downloading sniper deployment config..."
          uv run python -c "
          from huggingface_hub import hf_hub_download
          import os
          from pathlib import Path

          try:
              path = hf_hub_download(
                  repo_id=os.getenv('HF_REPO_ID', 'czlowiekZplanety/bettip-data'),
                  filename='config/sniper_deployment.json',
                  repo_type='dataset',
                  token=os.getenv('HF_TOKEN'),
                  local_dir='.'
              )
              print(f'Downloaded: {path}')
          except Exception as e:
              print(f'Warning: Could not download sniper config: {e}')
              print('Using default strategies.yaml')
          " || echo "Using strategies.yaml fallback"

      - name: Verify models synced
        run: |
          uv run python -c "
          import json, os
          config = json.load(open('config/sniper_deployment.json'))
          missing = []
          for name, mc in config.get('markets', {}).items():
              if not mc.get('enabled'): continue
              for m in (mc.get('saved_models') or []):
                  if not os.path.exists(f'models/{m}'):
                      missing.append(f'{name}: {m}')
          if missing:
              print(f'WARNING: {len(missing)} model files missing after download:')
              for m in missing: print(f'  - {m}')
              print('Re-downloading individual files...')
              from huggingface_hub import hf_hub_download
              token = os.getenv('HF_TOKEN')
              repo_id = os.getenv('HF_REPO_ID', 'czlowiekZplanety/bettip-data')
              fixed = 0
              for entry in missing:
                  fname = entry.split(': ', 1)[1]
                  try:
                      hf_hub_download(repo_id=repo_id, filename=f'models/{fname}',
                          repo_type='dataset', token=token, local_dir='.',
                          force_download=True)
                      fixed += 1
                      print(f'  Fixed: {fname}')
                  except Exception as e:
                      print(f'  FAILED: {fname} - {e}')
              print(f'Fixed {fixed}/{len(missing)} missing models')
          else:
              print('All model files present')
          print(f'Total models: {len(os.listdir(\"models\"))} files')
          "


      - name: Update previous results (CLV tracking)
        run: |
          echo "Updating results for previous predictions..."
          uv run python experiments/update_results.py 2>&1 || echo "Results update skipped (no previous predictions)"

      - name: Determine leagues
        id: leagues
        run: |
          LEAGUE_INPUT="${{ github.event.inputs.leagues || 'all' }}"
          if [ "$LEAGUE_INPUT" = "all" ]; then
            LEAGUES="${ALL_LEAGUES}"
          else
            LEAGUES="$LEAGUE_INPUT"
          fi
          echo "leagues=$LEAGUES" >> $GITHUB_OUTPUT

      - name: Fetch daily schedule (local parquet, 0 API calls)
        run: |
          echo "Loading today's schedule from local parquet files..."
          uv run python -m src.data_collection.match_scheduler --fetch --days-ahead 0 --local --leagues ${{ steps.leagues.outputs.leagues }}

      - name: Fetch referee assignments from API-Football
        continue-on-error: true
        run: |
          echo "Updating referee assignments for today's matches..."
          uv run python << 'PYEOF'
          import json
          import logging
          from collections import defaultdict
          from datetime import datetime
          from pathlib import Path

          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          from src.data_collection.api_client import FootballAPIClient
          from src.leagues import LEAGUE_IDS

          schedule_file = Path("data/06-prematch/today_schedule.json")
          if not schedule_file.exists():
              print("No schedule file, skipping referee fetch")
              exit(0)

          with open(schedule_file) as f:
              schedule = json.load(f)

          matches = schedule.get("matches", [])
          if not matches:
              print("No matches in schedule")
              exit(0)

          # Find matches with missing referee data
          missing_ref = [m for m in matches if not m.get("referee") or m["referee"] in ("None", "")]
          if not missing_ref:
              print("All matches already have referee data")
              exit(0)

          print(f"{len(missing_ref)}/{len(matches)} matches missing referee data")

          # Group by league to make one API call per league
          by_league = defaultdict(list)
          for m in missing_ref:
              by_league[m["league"]].append(m)

          client = FootballAPIClient()
          remaining = client.daily_limit - client.state.get("count", 0)
          print(f"API budget remaining: {remaining} requests")

          # Reserve budget for odds fetch and other steps
          max_calls = min(remaining - 50, len(by_league))
          if max_calls < 1:
              print("Not enough API budget for referee fetch")
              exit(0)

          # Build fixture_id lookup for fast matching
          fixture_lookup = {m["fixture_id"]: m for m in matches}

          # Fetch fixtures by league + date (1 call per league)
          today_str = schedule.get("date", "")
          updated_count = 0

          # API-Football uses season start year (e.g. 2025 = 2025-26 season)
          now = datetime.now()
          current_season = now.year if now.month >= 8 else now.year - 1

          for league_name, league_matches in list(by_league.items())[:max_calls]:
              league_id = LEAGUE_IDS.get(league_name)
              if not league_id:
                  logger.warning(f"Unknown league: {league_name}")
                  continue

              try:
                  response = client._make_request(
                      "/fixtures",
                      {"league": league_id, "season": current_season, "date": today_str}
                  )

                  for fix in response.get("response", []):
                      fixture_info = fix.get("fixture", {})
                      fid = fixture_info.get("id")
                      referee = fixture_info.get("referee") or ""

                      if fid in fixture_lookup and referee:
                          fixture_lookup[fid]["referee"] = referee
                          updated_count += 1

                  logger.info(f"{league_name}: fetched fixtures, updated referees")

              except Exception as e:
                  logger.error(f"Error fetching {league_name} fixtures: {e}")

          # Save updated schedule
          with open(schedule_file, "w") as f:
              json.dump(schedule, f, indent=2)

          print(f"Updated {updated_count} referee assignments ({len(by_league)} API calls)")
          PYEOF

      - name: Fetch pre-match odds from The Odds API
        continue-on-error: true
        env:
          THE_ODDS_API_KEY: ${{ secrets.THE_ODDS_API_KEY }}
        run: |
          if [ -n "$THE_ODDS_API_KEY" ]; then
            echo "Fetching pre-match odds (h2h + totals + niche markets)..."
            uv run python experiments/fetch_prematch_odds.py 2>&1 || echo "Odds fetch failed (non-critical)"
          else
            echo "THE_ODDS_API_KEY not set, skipping pre-match odds fetch"
          fi

      - name: Fetch pre-match odds from API-Football
        continue-on-error: true
        run: |
          echo "Fetching pre-match odds from API-Football (all markets, 1 call per fixture)..."
          uv run python << 'PYEOF'
          import json
          import logging
          from pathlib import Path

          logging.basicConfig(level=logging.INFO)

          from src.data_collection.api_client import FootballAPIClient
          from src.odds.api_football_odds_loader import ApiFootballOddsLoader

          schedule_file = Path("data/06-prematch/today_schedule.json")
          if not schedule_file.exists():
              print("No schedule file, skipping API-Football odds")
              exit(0)

          with open(schedule_file) as f:
              schedule = json.load(f)

          matches = schedule.get("matches", [])
          if not matches:
              print("No matches in schedule")
              exit(0)

          # Check remaining API budget
          client = FootballAPIClient()
          remaining = client.daily_limit - client.state.get("count", 0)
          print(f"API budget remaining: {remaining} requests")

          # Reserve ~20 calls for other steps, use rest for odds
          max_odds_calls = min(remaining - 20, len(matches))
          if max_odds_calls < 1:
              print("Not enough API budget for odds fetch")
              exit(0)

          # Build fixture_id ‚Üí team names mapping
          fixture_ids = []
          team_names = {}
          for m in matches[:max_odds_calls]:
              fid = m.get("fixture_id")
              if fid:
                  fixture_ids.append(fid)
                  team_names[fid] = (m["home_team"], m["away_team"])

          print(f"Fetching odds for {len(fixture_ids)} fixtures...")
          loader = ApiFootballOddsLoader()
          df = loader.fetch_and_save(fixture_ids, team_names=team_names)
          if not df.empty:
              print(f"Saved odds for {len(df)} fixtures")
          else:
              print("No odds returned from API-Football")
          PYEOF

      - name: Run ML predictions (local models only, 0 API calls)
        id: predict
        run: |
          echo "Running ML predictions (local only, saving API budget for pre-kickoff)..."
          uv run python -m src.data_collection.match_scheduler --predict --edge 0.05 --skip-api

          # Get counts
          if [ -f data/06-prematch/today_schedule.json ]; then
            TOTAL=$(uv run python -c "import json; print(json.load(open('data/06-prematch/today_schedule.json'))['total_matches'])")
            echo "total_matches=$TOTAL" >> $GITHUB_OUTPUT
          else
            echo "total_matches=0" >> $GITHUB_OUTPUT
          fi

          if [ -f data/06-prematch/interesting_matches.json ]; then
            INTERESTING=$(uv run python -c "import json; print(json.load(open('data/06-prematch/interesting_matches.json'))['interesting_count'])")
            echo "interesting_count=$INTERESTING" >> $GITHUB_OUTPUT
          else
            echo "interesting_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Generate recommendations CSV
        id: recommendations
        env:
          THE_ODDS_API_KEY: ${{ secrets.THE_ODDS_API_KEY }}
        run: |
          echo "Generating recommendations (sniper models)..."
          uv run python experiments/generate_daily_recommendations.py --min-edge 5 --schedule-file data/06-prematch/today_schedule.json 2>&1 | tee predictions_log.txt || true

          # Find generated file
          DATE=$(date +%Y%m%d)
          REC_FILE=$(ls -t data/05-recommendations/rec_${DATE}_*.csv 2>/dev/null | head -1)

          if [ -n "$REC_FILE" ]; then
            COUNT=$(wc -l < "$REC_FILE")
            echo "predictions_count=$((COUNT - 1))" >> $GITHUB_OUTPUT
            echo "predictions_file=$REC_FILE" >> $GITHUB_OUTPUT
          else
            echo "predictions_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Import to CLV tracker
        if: steps.recommendations.outputs.predictions_count > 0
        run: |
          REC_FILE="${{ steps.recommendations.outputs.predictions_file }}"
          if [ -n "$REC_FILE" ] && [ -f "$REC_FILE" ]; then
            uv run python experiments/clv_workflow.py import --file "$REC_FILE" 2>&1 || true
          fi

      - name: Import to prediction ledger
        if: steps.recommendations.outputs.predictions_count > 0
        run: |
          REC_FILE="${{ steps.recommendations.outputs.predictions_file }}"
          if [ -n "$REC_FILE" ] && [ -f "$REC_FILE" ]; then
            uv run python scripts/preds_ledger.py import --file "$REC_FILE" 2>&1 || true
          fi

      - name: Settle prediction ledger results
        continue-on-error: true
        run: uv run python scripts/preds_ledger.py settle 2>&1 || true

      # Niche odds dropped from weekends ‚Äî already fetched Mon-Thu.
      # Saves ~34 req for hourly lineup collection checks.

      - name: Upload prematch data to HF Hub
        continue-on-error: true
        run: |
          echo "Uploading prematch data to HF Hub..."
          uv run python -c "
          import os
          from huggingface_hub import HfApi
          from pathlib import Path

          api = HfApi(token=os.getenv('HF_TOKEN'))
          repo_id = os.getenv('HF_REPO_ID', 'czlowiekZplanety/bettip-data')

          for folder in ['data/05-recommendations', 'data/06-prematch', 'data/prematch_odds']:
              if Path(folder).exists():
                  api.upload_folder(
                      folder_path=folder,
                      path_in_repo=folder,
                      repo_id=repo_id,
                      repo_type='dataset',
                      commit_message=f'Update {folder} - $(date +%Y-%m-%d)'
                  )
                  print(f'Uploaded {folder}')
          "

      - name: Send Telegram summary
        continue-on-error: true
        if: steps.recommendations.outputs.predictions_count > 0
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          if [ -n "$TELEGRAM_BOT_TOKEN" ] && [ -n "$TELEGRAM_CHAT_ID" ]; then
            uv run python << 'PYEOF'
          import csv
          import json
          import os
          import requests
          from glob import glob
          from datetime import datetime

          # Read from sniper rec CSV (single source of truth)
          date_str = datetime.now().strftime("%Y%m%d")
          rec_files = sorted(glob(f"data/05-recommendations/rec_{date_str}_*.csv"))
          if not rec_files:
              print("No recommendation CSV found, skipping Telegram")
              exit(0)

          rec_file = rec_files[-1]  # latest batch
          with open(rec_file) as f:
              reader = csv.DictReader(f)
              recs = list(reader)

          # Also load schedule for total match count
          total_matches = 0
          if os.path.exists("data/06-prematch/today_schedule.json"):
              with open("data/06-prematch/today_schedule.json") as f:
                  total_matches = json.load(f).get("total_matches", 0)

          # Group by match, pick best edge per match
          match_best = {}
          for r in recs:
              key = (r["home_team"], r["away_team"])
              edge = float(r.get("edge", 0))
              if key not in match_best or edge > float(match_best[key].get("edge", 0)):
                  match_best[key] = r

          # Sort by edge descending, top 10
          top = sorted(match_best.values(), key=lambda x: float(x.get("edge", 0)), reverse=True)[:10]

          sep = "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          unique_matches = len(set((r["home_team"], r["away_team"]) for r in recs))
          lines = [
              "‚öΩ <b>DAILY PREDICTIONS</b>",
              sep,
              f"üìä {total_matches} matches ‚Ä¢ {len(recs)} picks from {unique_matches} fixtures",
              "",
          ]

          MARKET_LABELS = {
              'HOME_WIN': 'Home Win', 'AWAY_WIN': 'Away Win',
              'OVER25': 'Over 2.5', 'UNDER25': 'Under 2.5',
              'BTTS': 'BTTS Yes',
          }

          for r in top:
              market = r.get("market", "")
              edge = float(r.get("edge", 0))
              prob = float(r.get("probability", 0))
              odds = float(r.get("odds", 0))

              # Format market label
              label = MARKET_LABELS.get(market, market)
              if label == market:
                  # Parse niche: CORNERS_U11.5 -> Corners Under 11.5
                  parts = market.split("_")
                  if len(parts) >= 2:
                      stat = parts[0].capitalize()
                      direction = "Over" if parts[1].startswith("O") else "Under" if parts[1].startswith("U") else parts[1]
                      line_val = parts[1][1:] if len(parts[1]) > 1 else ""
                      label = f"{stat} {direction} {line_val}" if line_val else f"{stat} {direction}"
                  # Parse FOULS_UNDER_275 -> Fouls Under 27.5
                  if "UNDER" in market or "OVER" in market:
                      parts = market.split("_")
                      stat = parts[0].capitalize()
                      direction = "Under" if "UNDER" in market else "Over"
                      num = parts[-1]
                      try:
                          line_val = float(num) / 10
                          label = f"{stat} {direction} {line_val}"
                      except ValueError:
                          pass

              emoji = "üî•" if edge >= 15 else "‚ö°" if edge > 0 else "üìâ"
              lines.append(f"{emoji} <b>{r['home_team']} vs {r['away_team']}</b>")
              odds_str = f" @ {odds:.2f}" if odds > 0 else ""
              edge_str = f"+{edge:.1f}%" if edge > 0 else f"{edge:.1f}%"
              lines.append(f"   {label}{odds_str} | Edge: {edge_str} | Prob: {prob:.3f}")
              lines.append("")

          lines.append(sep)

          # Drift alerts from tracking signal
          try:
              import sys
              sys.path.insert(0, os.getcwd())
              from experiments.generate_daily_recommendations import compute_market_tracking_signals
              ts_signals = compute_market_tracking_signals()
              alerting = {m: s for m, s in ts_signals.items() if s.get("alert")}
              if alerting:
                  lines.append("")
                  lines.append("‚ö†Ô∏è <b>DRIFT ALERTS</b>")
                  for mkt, sig in sorted(alerting.items()):
                      emoji = "üìà" if sig["ts"] > 0 else "üìâ"
                      lines.append(f"{emoji} {mkt}: TS={sig['ts']:+.1f} ({sig['direction']}, n={sig['n_settled']})")
                  lines.append(sep)
          except Exception as e:
              print(f"Drift alert skipped: {e}")

          message = "\n".join(lines)
          token = os.environ.get('TELEGRAM_BOT_TOKEN')
          chat_id = os.environ.get('TELEGRAM_CHAT_ID')

          if token and chat_id:
              requests.post(
                  f"https://api.telegram.org/bot{token}/sendMessage",
                  data={'chat_id': chat_id, 'text': message, 'parse_mode': 'HTML'}
              )
              print(f"Telegram notification sent! ({len(recs)} recs, top {len(top)} shown)")
          PYEOF
          fi

      - name: Create summary
        if: always()
        run: |
          TOTAL="${{ steps.predict.outputs.total_matches || '0' }}"
          INTERESTING="${{ steps.predict.outputs.interesting_count || '0' }}"
          RECS="${{ steps.recommendations.outputs.predictions_count || '0' }}"

          echo "## Daily Predictions - $(date +%Y-%m-%d)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total matches | $TOTAL |" >> $GITHUB_STEP_SUMMARY
          echo "| Interesting (edge >= 5%) | $INTERESTING |" >> $GITHUB_STEP_SUMMARY
          echo "| Recommendations generated | $RECS |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f data/06-prematch/interesting_matches.json ]; then
            echo "### Top Predictions" >> $GITHUB_STEP_SUMMARY
            uv run python -c "
          import json
          with open('data/06-prematch/interesting_matches.json') as f:
              data = json.load(f)
          for m in data['matches'][:10]:
              pred = m.get('prediction', {})
              edge = pred.get('max_edge', 0) * 100
              market = pred.get('best_market')
              edge_details = pred.get('edge_details', {})
              if market and market in edge_details:
                  prob = edge_details[market].get('ml_prob', 0)
                  print(f'- **{m[\"home_team\"]} vs {m[\"away_team\"]}**: +{edge:.1f}% edge on {market} (prob: {prob:.3f})')
              elif edge_details:
                  top = max(edge_details.items(), key=lambda x: x[1].get('edge', 0))
                  prob = top[1].get('ml_prob', 0)
                  top_edge = top[1].get('edge', 0) * 100
                  print(f'- **{m[\"home_team\"]} vs {m[\"away_team\"]}**: {top_edge:+.1f}% on {top[0]} (prob: {prob:.3f})')
              else:
                  print(f'- **{m[\"home_team\"]} vs {m[\"away_team\"]}**: no predictions')
          " >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: daily-predictions-${{ github.run_number }}
          path: |
            data/06-prematch/
            data/05-recommendations/
            predictions_log.txt
          retention-days: 30

  # ==========================================================================
  # JOB 2: Lineup collection (every hour at :30)
  # - Check local schedule for matches 40-50 mins away
  # - Collect lineups for interesting matches only
  # - Send Telegram alert
  # ==========================================================================
  collect-lineups:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'schedule' && github.event.schedule == '30 11-20 * * 5,6,0')

    steps:
      - uses: actions/checkout@v4
      - name: Setup Python environment
        uses: ./.github/actions/setup-python-uv
        with:
          extras: dl

      - name: Create .env file
        run: |
          echo "API_FOOTBALL_KEY=${{ secrets.API_FOOTBALL_KEY }}" > .env
          echo "HF_TOKEN=${{ secrets.HF_TOKEN }}" >> .env
          echo "DAILY_LIMIT=500" >> .env
          echo "PER_MIN_LIMIT=30" >> .env

      - name: Download data from HF Hub
        run: |
          echo "Downloading prematch data, models, and deployment config..."
          uv run python entrypoints/download_data.py
          uv run python -c "
          from huggingface_hub import hf_hub_download, snapshot_download
          import os

          repo_id = os.getenv('HF_REPO_ID', 'czlowiekZplanety/bettip-data')
          token = os.getenv('HF_TOKEN')

          # Download prematch data
          snapshot_download(
              repo_id=repo_id, repo_type='dataset', local_dir='.',
              allow_patterns=['data/06-prematch/**', 'data/05-recommendations/**'],
              token=token, force_download=True
          )
          print('Downloaded prematch data')

          # Download deployment config
          try:
              hf_hub_download(repo_id=repo_id, filename='config/sniper_deployment.json',
                  repo_type='dataset', token=token, local_dir='.')
              print('Downloaded sniper deployment config')
          except Exception as e:
              print(f'Warning: {e}')
          " || echo "No prematch data in HF yet"

      - name: Check if collection needed
        id: check
        run: |
          uv run python -c "
          import json
          import sys
          from datetime import datetime, timezone, timedelta
          from pathlib import Path

          schedule_file = Path('data/06-prematch/today_schedule.json')
          interesting_file = Path('data/06-prematch/interesting_matches.json')

          if not schedule_file.exists():
              print('No schedule file - skipping')
              sys.exit(0)

          with open(schedule_file) as f:
              schedule = json.load(f)

          interesting_ids = set()
          if interesting_file.exists():
              with open(interesting_file) as f:
                  interesting = json.load(f)
              interesting_ids = {m['fixture_id'] for m in interesting.get('matches', [])}
              print(f'Loaded {len(interesting_ids)} interesting matches')

          now = datetime.now(timezone.utc)
          window_start = now + timedelta(minutes=15)
          window_end = now + timedelta(minutes=70)

          matches_in_window = []
          for m in schedule.get('matches', []):
              kickoff = datetime.fromisoformat(m['kickoff'])
              if window_start <= kickoff <= window_end:
                  if not interesting_ids or m['fixture_id'] in interesting_ids:
                      mins = int((kickoff - now).total_seconds() / 60)
                      m['mins_until'] = mins
                      matches_in_window.append(m)

          if matches_in_window:
              print(f'COLLECT NOW! {len(matches_in_window)} match(es):')
              for m in matches_in_window:
                  print(f'  {m[\"home_team\"]} vs {m[\"away_team\"]} in {m[\"mins_until\"]} mins')
              with open('matches_to_collect.json', 'w') as f:
                  json.dump(matches_in_window, f)
          else:
              print('No interesting matches in lineup window')
          "

          if [ -f matches_to_collect.json ]; then
            echo "should_collect=true" >> $GITHUB_OUTPUT
          else
            echo "should_collect=false" >> $GITHUB_OUTPUT
          fi

      - name: Collect lineups and re-predict
        continue-on-error: true
        if: steps.check.outputs.should_collect == 'true'
        id: collect
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          set -o pipefail
          echo "Running pre-kickoff re-prediction (lineup fetch + model re-run)..."
          uv run python scripts/pre_kickoff_repredict.py --api-budget 50 --delta-threshold 0.02 2>&1 | tee pre_kickoff_log.txt

          # Check if updated predictions were generated
          DATE=$(date +%Y%m%d)
          if [ -f "data/05-recommendations/pre_kickoff_${DATE}.csv" ]; then
            COUNT=$(wc -l < "data/05-recommendations/pre_kickoff_${DATE}.csv")
            echo "has_updates=true" >> $GITHUB_OUTPUT
            echo "update_count=$((COUNT - 1))" >> $GITHUB_OUTPUT
          else
            echo "has_updates=false" >> $GITHUB_OUTPUT
            echo "update_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Send lineup collection Telegram summary
        continue-on-error: true
        if: steps.check.outputs.should_collect == 'true'
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          if [ -n "$TELEGRAM_BOT_TOKEN" ] && [ -n "$TELEGRAM_CHAT_ID" ]; then
            HAS_UPDATES="${{ steps.collect.outputs.has_updates || 'false' }}"
            UPDATE_COUNT="${{ steps.collect.outputs.update_count || '0' }}"

            uv run python << PYEOF
          import json
          import os
          import requests
          from pathlib import Path

          sep = "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          has_updates = "$HAS_UPDATES" == "true"
          update_count = int("$UPDATE_COUNT" or "0")

          # Count matches checked
          matches_file = Path("matches_to_collect.json")
          match_count = 0
          match_names = []
          if matches_file.exists():
              with open(matches_file) as f:
                  matches = json.load(f)
              match_count = len(matches)
              match_names = [f"{m['home_team']} vs {m['away_team']}" for m in matches[:5]]

          lines = [
              "üìã <b>LINEUP COLLECTION</b>",
              sep,
              f"üîç Checked {match_count} match(es) for lineups",
          ]

          for name in match_names:
              lines.append(f"  ‚Ä¢ {name}")

          if has_updates and update_count > 0:
              lines.append(f"")
              lines.append(f"‚ö° {update_count} prediction(s) updated after lineup data")
          else:
              lines.append(f"")
              lines.append(f"üìä No significant probability changes (delta < 2pp)")

          lines.append(sep)
          message = "\n".join(lines)

          token = os.environ.get("TELEGRAM_BOT_TOKEN")
          chat_id = os.environ.get("TELEGRAM_CHAT_ID")
          if token and chat_id:
              requests.post(
                  f"https://api.telegram.org/bot{token}/sendMessage",
                  data={"chat_id": chat_id, "text": message, "parse_mode": "HTML"},
              )
              print("Lineup collection Telegram notification sent!")
          PYEOF
          fi

      - name: Upload pre-kickoff data to HF Hub
        continue-on-error: true
        if: steps.check.outputs.should_collect == 'true'
        run: |
          uv run python -c "
          import os
          from huggingface_hub import HfApi
          from pathlib import Path

          api = HfApi(token=os.getenv('HF_TOKEN'))
          repo_id = os.getenv('HF_REPO_ID', 'czlowiekZplanety/bettip-data')

          for folder in ['data/05-recommendations', 'data/06-prematch']:
              if Path(folder).exists():
                  api.upload_folder(
                      folder_path=folder, path_in_repo=folder,
                      repo_id=repo_id, repo_type='dataset',
                      commit_message=f'Pre-kickoff update {folder}'
                  )
                  print(f'Uploaded {folder}')
          "

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pre-kickoff-${{ github.run_number }}
          path: |
            data/06-prematch/
            data/05-recommendations/
            pre_kickoff_log.txt
          retention-days: 30
