name: Fetch The Odds API Data

on:
  schedule:
    # Run daily at 6 AM UTC (before matches typically start)
    - cron: '0 6 * * *'

  workflow_dispatch:
    inputs:
      leagues:
        description: 'Leagues to fetch (comma-separated or "all")'
        required: false
        default: 'all'
        type: string
      markets:
        description: 'Markets to fetch'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - btts
          - corners
          - cards
          - shots
      mode:
        description: 'Fetch mode'
        required: false
        default: 'current'
        type: choice
        options:
          - current
          - historical
      historical_date:
        description: 'Date for historical mode (YYYY-MM-DD)'
        required: false
        default: ''
        type: string

permissions:
  contents: read

env:
  ALL_LEAGUES: "premier_league,la_liga,serie_a,bundesliga,ligue_1"

jobs:
  fetch-odds:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Verify secrets
        run: |
          if [ -z "${{ secrets.HF_TOKEN }}" ]; then
            echo "::error::HF_TOKEN secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.THE_ODDS_API_KEY }}" ]; then
            echo "::error::THE_ODDS_API_KEY secret is not set"
            exit 1
          fi
          echo "All required secrets are configured"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Cache UV dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Set up Python
        run: uv python install 3.10

      - name: Install dependencies
        run: uv sync --frozen

      - name: Create .env file
        run: |
          echo "THE_ODDS_API_KEY=${{ secrets.THE_ODDS_API_KEY }}" > .env
          echo "HF_TOKEN=${{ secrets.HF_TOKEN }}" >> .env

      - name: Create output directories
        run: |
          mkdir -p data/theodds_cache
          mkdir -p data/03-features

      - name: Check The Odds API status
        run: |
          uv run python -c "
          from src.odds.theodds_unified_loader import TheOddsUnifiedLoader
          import json

          loader = TheOddsUnifiedLoader()
          status = loader.check_api_status()

          print('=== The Odds API Status ===')
          print(json.dumps(status, indent=2))

          if status.get('status') != 'ok':
              print('::error::API connection failed')
              exit(1)

          remaining = int(status.get('requests_remaining', 0))
          if remaining < 100:
              print(f'::warning::Low API quota: {remaining} requests remaining')
          "

      - name: Determine leagues and markets
        id: params
        run: |
          LEAGUES_INPUT="${{ github.event.inputs.leagues || 'all' }}"
          MARKETS_INPUT="${{ github.event.inputs.markets || 'all' }}"

          if [ "$LEAGUES_INPUT" = "all" ]; then
            LEAGUES="${ALL_LEAGUES}"
          else
            LEAGUES="$LEAGUES_INPUT"
          fi

          echo "leagues=$LEAGUES" >> $GITHUB_OUTPUT
          echo "markets=$MARKETS_INPUT" >> $GITHUB_OUTPUT
          echo "Processing leagues: $LEAGUES"
          echo "Processing markets: $MARKETS_INPUT"

      - name: Fetch current odds
        if: github.event.inputs.mode != 'historical'
        run: |
          LEAGUES="${{ steps.params.outputs.leagues }}"
          MARKETS="${{ steps.params.outputs.markets }}"

          uv run python -c "
          import logging
          logging.basicConfig(level=logging.INFO)

          from src.odds.theodds_unified_loader import TheOddsUnifiedLoader
          import pandas as pd
          from pathlib import Path

          loader = TheOddsUnifiedLoader()
          leagues = '$LEAGUES'.split(',')
          markets = '$MARKETS'

          all_odds = []

          for league in leagues:
              league = league.strip()
              print(f'\n=== Fetching {league} ===')

              try:
                  if markets == 'all':
                      df = loader.fetch_all_markets(league, save_to_cache=True)
                  else:
                      df = loader.fetch_market(league, markets, save_to_cache=True)

                  if not df.empty:
                      all_odds.append(df)
                      print(f'  Fetched {len(df)} matches')

                      # Show market coverage
                      for market in ['btts', 'corners', 'cards', 'shots']:
                          col = f'{market}_' if market != 'btts' else 'btts_'
                          cols = [c for c in df.columns if c.startswith(col)]
                          if cols:
                              coverage = df[cols].notna().any(axis=1).sum()
                              print(f'    {market}: {coverage}/{len(df)} matches')
                  else:
                      print(f'  No odds available')
              except Exception as e:
                  print(f'  Error: {e}')

          # Combine all leagues
          if all_odds:
              combined = pd.concat(all_odds, ignore_index=True)
              output_path = Path('data/theodds_cache/all_leagues_current.parquet')
              combined.to_parquet(output_path, index=False)
              print(f'\n=== Combined Results ===')
              print(f'Total: {len(combined)} matches from {len(all_odds)} leagues')
              print(f'Saved to: {output_path}')

              # Also save CSV for debugging
              combined.to_csv(output_path.with_suffix('.csv'), index=False)
          "

      - name: Fetch historical odds
        if: github.event.inputs.mode == 'historical' && github.event.inputs.historical_date != ''
        run: |
          LEAGUES="${{ steps.params.outputs.leagues }}"
          MARKETS="${{ steps.params.outputs.markets }}"
          DATE="${{ github.event.inputs.historical_date }}"

          uv run python -c "
          import logging
          logging.basicConfig(level=logging.INFO)

          from src.odds.theodds_unified_loader import TheOddsUnifiedLoader
          import pandas as pd
          from pathlib import Path

          loader = TheOddsUnifiedLoader()
          leagues = '$LEAGUES'.split(',')
          date = '$DATE'
          markets_list = None if '$MARKETS' == 'all' else ['$MARKETS']

          all_odds = []

          for league in leagues:
              league = league.strip()
              print(f'\n=== Fetching {league} for {date} ===')

              try:
                  df = loader.fetch_historical_odds(league, date, markets=markets_list)
                  if not df.empty:
                      all_odds.append(df)
                      print(f'  Fetched {len(df)} matches')
                  else:
                      print(f'  No odds available')
              except Exception as e:
                  print(f'  Error: {e}')

          if all_odds:
              combined = pd.concat(all_odds, ignore_index=True)
              output_path = Path(f'data/theodds_cache/historical_{date}.parquet')
              combined.to_parquet(output_path, index=False)
              print(f'\nSaved {len(combined)} matches to {output_path}')
          "

      - name: Generate estimated fouls odds
        run: |
          uv run python -c "
          import logging
          logging.basicConfig(level=logging.INFO)

          from src.odds.fouls_odds_loader import FoulsOddsLoader
          import pandas as pd
          from pathlib import Path

          # Load current odds if available
          odds_file = Path('data/theodds_cache/all_leagues_current.parquet')
          if odds_file.exists():
              df = pd.read_parquet(odds_file)
              print(f'Loaded {len(df)} matches from The Odds API')

              # Add estimated fouls odds (no real provider offers this)
              loader = FoulsOddsLoader()
              df_with_fouls = loader.estimate_for_upcoming(df)

              # Save updated file
              df_with_fouls.to_parquet(odds_file, index=False)
              df_with_fouls.to_csv(odds_file.with_suffix('.csv'), index=False)
              print(f'Added estimated fouls odds to {len(df_with_fouls)} matches')
          else:
              print('No current odds file found, skipping fouls estimation')
          "

      - name: Show API usage summary
        run: |
          uv run python -c "
          from src.odds.theodds_unified_loader import TheOddsUnifiedLoader

          loader = TheOddsUnifiedLoader()
          status = loader.check_api_status()

          print('=== API Usage Summary ===')
          print(f'Requests used: {status.get(\"requests_used\", \"?\")}')
          print(f'Requests remaining: {status.get(\"requests_remaining\", \"?\")}')
          "

      - name: Upload to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          uv run python -c "
          from huggingface_hub import HfApi
          import os
          from pathlib import Path

          api = HfApi(token=os.getenv('HF_TOKEN'))
          cache_dir = Path('data/theodds_cache')

          uploaded = 0
          for f in cache_dir.glob('*.parquet'):
              print(f'Uploading {f.name}...')
              api.upload_file(
                  path_or_fileobj=str(f),
                  path_in_repo=f'data/theodds_odds/{f.name}',
                  repo_id='czlowiekZplanety/bettip-data',
                  repo_type='dataset',
                  commit_message=f'Update The Odds API data: {f.name}'
              )
              uploaded += 1

          print(f'Uploaded {uploaded} files to HF Hub')
          "

      - name: Create summary
        if: always()
        run: |
          echo "## The Odds API Fetch Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          LEAGUES="${{ steps.params.outputs.leagues }}"
          MARKETS="${{ steps.params.outputs.markets }}"
          MODE="${{ github.event.inputs.mode || 'current' }}"

          echo "### Parameters" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Mode | \`$MODE\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Leagues | \`$LEAGUES\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Markets | \`$MARKETS\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Files Generated" >> $GITHUB_STEP_SUMMARY
          echo "| File | Size | Matches |" >> $GITHUB_STEP_SUMMARY
          echo "|------|------|---------|" >> $GITHUB_STEP_SUMMARY

          for f in data/theodds_cache/*.parquet; do
            if [ -f "$f" ]; then
              SIZE=$(du -h "$f" | cut -f1)
              NAME=$(basename "$f")
              ROWS=$(uv run python -c "import pandas as pd; print(len(pd.read_parquet('$f')))" 2>/dev/null || echo "?")
              echo "| \`$NAME\` | $SIZE | $ROWS |" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Market Coverage" >> $GITHUB_STEP_SUMMARY

          if [ -f "data/theodds_cache/all_leagues_current.parquet" ]; then
            uv run python -c "
          import pandas as pd

          df = pd.read_parquet('data/theodds_cache/all_leagues_current.parquet')
          total = len(df)

          print('| Market | Coverage |')
          print('|--------|----------|')

          for market in ['btts', 'corners', 'cards', 'shots', 'fouls']:
              col = f'{market}_' if market != 'btts' else 'btts_'
              cols = [c for c in df.columns if c.startswith(col)]
              if cols:
                  coverage = df[cols].notna().any(axis=1).sum()
                  pct = coverage / total * 100
                  print(f'| {market.upper()} | {coverage}/{total} ({pct:.0f}%) |')
          " >> $GITHUB_STEP_SUMMARY || echo "Could not calculate coverage"
          fi

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: theodds-data
          path: |
            data/theodds_cache/
          retention-days: 30
